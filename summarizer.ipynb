{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\ianwa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error while downloading from https://cdn-lfs.hf.co/facebook/bart-large-cnn/40041830399afb5348525ef8354b007ecec4286fdf3524f7e6b54377e17096cb?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27model.safetensors%3B+filename%3D%22model.safetensors%22%3B&Expires=1739501882&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTczOTUwMTg4Mn19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5oZi5jby9mYWNlYm9vay9iYXJ0LWxhcmdlLWNubi80MDA0MTgzMDM5OWFmYjUzNDg1MjVlZjgzNTRiMDA3ZWNlYzQyODZmZGYzNTI0ZjdlNmI1NDM3N2UxNzA5NmNiP3Jlc3BvbnNlLWNvbnRlbnQtZGlzcG9zaXRpb249KiJ9XX0_&Signature=KIkV7LAM8tGgmRPypOohGxPI1ezFPvMCHZxUtPI3Xwi1D7kQDfVK1E358jJHBwmo5mEvzYRxVTB7ZQwU0qqrSSDU2YtzaWnDxKFLWjwc4A1zhWvOIE85yVQdRE7t6E86k1cwSzjK%7EtthdCbkpPx0liyLOW-iqH-1GPOzzQ06t0VuuiFWZeUGVGeVMdmUHSwrYMetpRPbCnnv70mo8Pi87RRPM222f5BXCQ0Mknty5xgCmp-GdZDSdS-HH9fg6cfK42b24EyRfbOEx8OFtYmyrxuVtppV%7E1RIjezUtuNMyEVhdj7NXfFsFnTuF61dzMKat6REBPlcSJHA71M30awKDw__&Key-Pair-Id=K3RPWS32NSSJCE: HTTPSConnectionPool(host='cdn-lfs.hf.co', port=443): Read timed out.\n",
      "Trying to resume download...\n",
      "c:\\Users\\ianwa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\ianwa\\.cache\\huggingface\\hub\\models--facebook--bart-large-cnn. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\ianwa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBartForConditionalGeneration.\n",
      "\n",
      "All the weights of TFBartForConditionalGeneration were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBartForConditionalGeneration for predictions without further training.\n",
      "Device set to use 0\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "\n",
    "ARTICLE = \"\"\" New York (CNN)When Liana Barrientos was 23 years old, she got married in Westchester County, New York.\n",
    "A year later, she got married again in Westchester County, but to a different man and without divorcing her first husband.\n",
    "Only 18 days after that marriage, she got hitched yet again. Then, Barrientos declared \"I do\" five more times, sometimes only within two weeks of each other.\n",
    "In 2010, she married once more, this time in the Bronx. In an application for a marriage license, she stated it was her \"first and only\" marriage.\n",
    "Barrientos, now 39, is facing two criminal counts of \"offering a false instrument for filing in the first degree,\" referring to her false statements on the\n",
    "2010 marriage license application, according to court documents.\n",
    "Prosecutors said the marriages were part of an immigration scam.\n",
    "On Friday, she pleaded not guilty at State Supreme Court in the Bronx, according to her attorney, Christopher Wright, who declined to comment further.\n",
    "After leaving court, Barrientos was arrested and charged with theft of service and criminal trespass for allegedly sneaking into the New York subway through an emergency exit, said Detective\n",
    "Annette Markowski, a police spokeswoman. In total, Barrientos has been married 10 times, with nine of her marriages occurring between 1999 and 2002.\n",
    "All occurred either in Westchester County, Long Island, New Jersey or the Bronx. She is believed to still be married to four men, and at one time, she was married to eight men at once, prosecutors say.\n",
    "Prosecutors said the immigration scam involved some of her husbands, who filed for permanent residence status shortly after the marriages.\n",
    "Any divorces happened only after such filings were approved. It was unclear whether any of the men will be prosecuted.\n",
    "The case was referred to the Bronx District Attorney\\'s Office by Immigration and Customs Enforcement and the Department of Homeland Security\\'s\n",
    "Investigation Division. Seven of the men are from so-called \"red-flagged\" countries, including Egypt, Turkey, Georgia, Pakistan and Mali.\n",
    "Her eighth husband, Rashid Rajput, was deported in 2006 to his native Pakistan after an investigation by the Joint Terrorism Task Force.\n",
    "If convicted, Barrientos faces up to four years in prison.  Her next court appearance is scheduled for May 18.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'summary_text': 'Liana Barrientos, 39, is charged with two counts of \"offering a false instrument for filing in the first degree\" In total, she has been married 10 times, with nine of her marriages occurring between 1999 and 2002. She is believed to still be married to four men.'}]\n"
     ]
    }
   ],
   "source": [
    "print(summarizer(ARTICLE, max_length=130, min_length=30, do_sample=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying out new code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ianwa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"argilla/news-summary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ianwa\\AppData\\Local\\Temp\\ipykernel_11964\\2027293581.py:6: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  df_train['prediction'][i] = df_train['prediction'][i][0]['text']\n",
      "C:\\Users\\ianwa\\AppData\\Local\\Temp\\ipykernel_11964\\2027293581.py:11: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  df_test['prediction'][i] = df_test['prediction'][i][0]['text']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_train = pd.DataFrame(ds['test'])\n",
    "df_train = df_train.drop(columns=[ 'prediction_agent', 'annotation', 'annotation_agent', 'id' ,'metadata', 'status', 'event_timestamp', 'metrics'])\n",
    "for i in range(len(df_train)):\n",
    "    df_train['prediction'][i] = df_train['prediction'][i][0]['text']\n",
    "\n",
    "df_test = pd.DataFrame(ds['train'])\n",
    "df_test = df_test.drop(columns=[ 'prediction_agent', 'annotation', 'annotation_agent', 'id' ,'metadata', 'status', 'event_timestamp', 'metrics'])\n",
    "for i in range(len(df_test)):\n",
    "    df_test['prediction'][i] = df_test['prediction'][i][0]['text']\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_train, df_val = train_test_split(df_train, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of df train: 16333\n",
      "len of df validation: 4084\n",
      "len of df test: 1000\n"
     ]
    }
   ],
   "source": [
    "print('len of df train:', len(df_train))\n",
    "print('len of df validation:', len(df_val))\n",
    "print('len of df test:', len(df_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample of df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_content</th>\n",
       "      <th>article_summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14764</th>\n",
       "      <td>BERLIN (Reuters) - The German military s procu...</td>\n",
       "      <td>Report shows 1,300 unfilled jobs, strain for G...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7537</th>\n",
       "      <td>WASHINGTON (Reuters) - U.S.-backed militias fi...</td>\n",
       "      <td>U.S.-backed forces not planning on entering De...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16178</th>\n",
       "      <td>SOCHI, Russia (Reuters) - Russian President Vl...</td>\n",
       "      <td>Putin says Russia will respond if Russian medi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6161</th>\n",
       "      <td>JACKSON, Ga. (Reuters) - Down a Georgia count...</td>\n",
       "      <td>U.S. militia girds for trouble as presidential...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1090</th>\n",
       "      <td>SEOUL (Reuters) - The escalating threat arisin...</td>\n",
       "      <td>North Korea missile crisis seen pushing South ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11284</th>\n",
       "      <td>HONOLULU (Reuters) - A federal judge in Hawaii...</td>\n",
       "      <td>Federal judge in Hawaii extends court order bl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11964</th>\n",
       "      <td>VIENNA (Reuters) - Austria s conservative Peop...</td>\n",
       "      <td>Austria's conservatives reach coalition deal w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5390</th>\n",
       "      <td>KANDAHAR, Afghanistan (Reuters) - A roadside b...</td>\n",
       "      <td>Six civilians killed by roadside bomb in Afgha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>860</th>\n",
       "      <td>ROME (Reuters) - The Italian Chamber of Deputi...</td>\n",
       "      <td>Italy lower house passes new electoral law, mo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15795</th>\n",
       "      <td>WASHINGTON (Reuters) - With no palatable milit...</td>\n",
       "      <td>Trump seeks tougher sanctions to prod North Ko...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16333 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         article_content  \\\n",
       "14764  BERLIN (Reuters) - The German military s procu...   \n",
       "7537   WASHINGTON (Reuters) - U.S.-backed militias fi...   \n",
       "16178  SOCHI, Russia (Reuters) - Russian President Vl...   \n",
       "6161    JACKSON, Ga. (Reuters) - Down a Georgia count...   \n",
       "1090   SEOUL (Reuters) - The escalating threat arisin...   \n",
       "...                                                  ...   \n",
       "11284  HONOLULU (Reuters) - A federal judge in Hawaii...   \n",
       "11964  VIENNA (Reuters) - Austria s conservative Peop...   \n",
       "5390   KANDAHAR, Afghanistan (Reuters) - A roadside b...   \n",
       "860    ROME (Reuters) - The Italian Chamber of Deputi...   \n",
       "15795  WASHINGTON (Reuters) - With no palatable milit...   \n",
       "\n",
       "                                         article_summary  \n",
       "14764  Report shows 1,300 unfilled jobs, strain for G...  \n",
       "7537   U.S.-backed forces not planning on entering De...  \n",
       "16178  Putin says Russia will respond if Russian medi...  \n",
       "6161   U.S. militia girds for trouble as presidential...  \n",
       "1090   North Korea missile crisis seen pushing South ...  \n",
       "...                                                  ...  \n",
       "11284  Federal judge in Hawaii extends court order bl...  \n",
       "11964  Austria's conservatives reach coalition deal w...  \n",
       "5390   Six civilians killed by roadside bomb in Afgha...  \n",
       "860    Italy lower house passes new electoral law, mo...  \n",
       "15795  Trump seeks tougher sanctions to prod North Ko...  \n",
       "\n",
       "[16333 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = df_train.rename(columns={\"text\": \"article_content\", \"prediction\": \"article_summary\"})\n",
    "df_test = df_test.rename(columns={\"text\": \"article_content\", \"prediction\": \"article_summary\"})\n",
    "df_val = df_val.rename(columns={\"text\": \"article_content\", \"prediction\": \"article_summary\"})\n",
    "df_train\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine tuned BART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class SummaryDataset(Dataset):\n",
    "    # Initialize the dataset with a tokenizer, data, and maximum token length\n",
    "    def __init__(self, tokenizer, data, max_length=512):\n",
    "        self.tokenizer = tokenizer  # Tokenizer for encoding text\n",
    "        self.data = data            # Data containing dialogues and summaries\n",
    "        self.max_length = max_length # Maximum length of tokens\n",
    "\n",
    "    # Return the number of items in the dataset\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    # Retrieve an item from the dataset by index\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data.iloc[idx]  # Get the row at the specified index\n",
    "        dialogue = item['article_content'] # Extract dialogue from the row\n",
    "        summary = item['article_summary']   # Extract summary from the row\n",
    "\n",
    "        # Encode the dialogue as input data for the model\n",
    "        source = self.tokenizer.encode_plus(\n",
    "            dialogue, \n",
    "            max_length=self.max_length, \n",
    "            padding='max_length', \n",
    "            return_tensors='pt', \n",
    "            truncation=True\n",
    "        )\n",
    "\n",
    "        # Encode the summary as target data for the model\n",
    "        target = self.tokenizer.encode_plus(\n",
    "            summary, \n",
    "            max_length=self.max_length, \n",
    "            padding='max_length', \n",
    "            return_tensors='pt', \n",
    "            truncation=True\n",
    "        )\n",
    "\n",
    "        # Return a dictionary containing input_ids, attention_mask, labels, and the original summary text\n",
    "        return {\n",
    "            'input_ids': source['input_ids'].flatten(),\n",
    "            'attention_mask': source['attention_mask'].flatten(),\n",
    "            'labels': target['input_ids'].flatten(),\n",
    "            'summary': summary \n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "\n",
    "# Initialize the tokenizer for BART\n",
    "# 'facebook/bart-base' is a pretrained model identifier\n",
    "# The tokenizer is responsible for converting text input into tokens that the model can understand\n",
    "tokenizer = BartTokenizer.from_pretrained('facebook/bart-base')\n",
    "\n",
    "# Initialize the BART model for conditional generation\n",
    "# This model is used for tasks like summarization where the output is conditional on the input text\n",
    "# The model is loaded with pretrained weights from 'facebook/bart-base'\n",
    "model = BartForConditionalGeneration.from_pretrained('facebook/bart-base')\n",
    "\n",
    "# Creating an instance of the SummaryDataset class for training data\n",
    "# It uses the tokenizer to process the training data (train_df) \n",
    "# for model input\n",
    "train_dataset = SummaryDataset(tokenizer, df_train)\n",
    "\n",
    "# Creating an instance of the SummaryDataset class for validation data\n",
    "# It uses the same tokenizer but with a different dataset (test_df2) \n",
    "# for validation purposes\n",
    "val_dataset = SummaryDataset(tokenizer, df_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\ianwa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ianwa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='23' max='4084' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  23/4084 38:23 < 123:42:52, 0.01 it/s, Epoch 0.01/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 26\u001b[0m\n\u001b[0;32m     18\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[0;32m     19\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,             \u001b[38;5;66;03m# The model to be trained (e.g., our BART model)\u001b[39;00m\n\u001b[0;32m     20\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,      \u001b[38;5;66;03m# Training arguments specifying training parameters like learning rate, batch size, etc.\u001b[39;00m\n\u001b[0;32m     21\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mtrain_dataset,  \u001b[38;5;66;03m# The dataset to be used for training the model\u001b[39;00m\n\u001b[0;32m     22\u001b[0m     eval_dataset\u001b[38;5;241m=\u001b[39mval_dataset    \u001b[38;5;66;03m# The dataset to be used for evaluating the model during training\u001b[39;00m\n\u001b[0;32m     23\u001b[0m )\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Starting the training process\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ianwa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\trainer.py:2171\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2169\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2170\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2171\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2172\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2175\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2176\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ianwa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\trainer.py:2531\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2524\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   2525\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[0;32m   2526\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   2527\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m!=\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED\n\u001b[0;32m   2528\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[0;32m   2529\u001b[0m )\n\u001b[0;32m   2530\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[1;32m-> 2531\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2533\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2534\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   2535\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m   2536\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   2537\u001b[0m ):\n\u001b[0;32m   2538\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   2539\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32mc:\\Users\\ianwa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\trainer.py:3675\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[0;32m   3672\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m   3674\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[1;32m-> 3675\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3677\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[0;32m   3678\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   3679\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   3680\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m   3681\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\ianwa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\trainer.py:3731\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[1;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[0;32m   3729\u001b[0m         loss_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_items_in_batch\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m num_items_in_batch\n\u001b[0;32m   3730\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mloss_kwargs}\n\u001b[1;32m-> 3731\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3732\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[0;32m   3733\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[0;32m   3734\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\ianwa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ianwa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\ianwa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\models\\bart\\modeling_bart.py:1659\u001b[0m, in \u001b[0;36mBartForConditionalGeneration.forward\u001b[1;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1637\u001b[0m         decoder_input_ids \u001b[38;5;241m=\u001b[39m shift_tokens_right(\n\u001b[0;32m   1638\u001b[0m             labels, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpad_token_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mdecoder_start_token_id\n\u001b[0;32m   1639\u001b[0m         )\n\u001b[0;32m   1641\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(\n\u001b[0;32m   1642\u001b[0m     input_ids,\n\u001b[0;32m   1643\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1656\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[0;32m   1657\u001b[0m )\n\u001b[1;32m-> 1659\u001b[0m lm_logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlm_head\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1660\u001b[0m lm_logits \u001b[38;5;241m=\u001b[39m lm_logits \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinal_logits_bias\u001b[38;5;241m.\u001b[39mto(lm_logits\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m   1662\u001b[0m masked_lm_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ianwa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ianwa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\ianwa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "# Define training arguments for the model\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # Directory to save model output and checkpoints\n",
    "    num_train_epochs=2,              # Number of epochs to train the model\n",
    "    per_device_train_batch_size=8,   # Batch size per device during training\n",
    "    per_device_eval_batch_size=8,    # Batch size for evaluation\n",
    "    warmup_steps=500,                # Number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # Weight decay for regularization\n",
    "    logging_dir='./logs',            # Directory to save logs\n",
    "    logging_steps=10,                # Log metrics every specified number of steps\n",
    "    evaluation_strategy=\"epoch\",     # Evaluation is done at the end of each epoch\n",
    "    report_to='none'                 # Disables reporting to any online services (e.g., TensorBoard, WandB)\n",
    ")\n",
    "\n",
    "# Initializing the Trainer object\n",
    "trainer = Trainer(\n",
    "    model=model,             # The model to be trained (e.g., our BART model)\n",
    "    args=training_args,      # Training arguments specifying training parameters like learning rate, batch size, etc.\n",
    "    train_dataset=train_dataset,  # The dataset to be used for training the model\n",
    "    eval_dataset=val_dataset    # The dataset to be used for evaluating the model during training\n",
    ")\n",
    "\n",
    "# Starting the training process\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation portion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "\n",
    "# Load the ROUGE metric for evaluation\n",
    "rouge = load_metric('rouge')\n",
    "\n",
    "def generate_summaries(model, tokenizer, dataset, batch_size=8):\n",
    "    \"\"\"\n",
    "    Generate summaries using the provided model and tokenizer on the given dataset.\n",
    "\n",
    "    Args:\n",
    "        model: The trained summarization model.\n",
    "        tokenizer: Tokenizer associated with the model.\n",
    "        dataset: Dataset for which summaries need to be generated.\n",
    "        batch_size: Number of data samples to process in each batch.\n",
    "\n",
    "    Returns:\n",
    "        summaries: Generated summaries by the model.\n",
    "        references: Actual summaries from the dataset for comparison.\n",
    "    \"\"\"\n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "    summaries = []    # List to store generated summaries\n",
    "    references = []   # List to store actual summaries\n",
    "\n",
    "    # Create a DataLoader for batch processing\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size)\n",
    "\n",
    "    # Disabled gradient calculations for efficiency\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            # Move input data to the same device as the model\n",
    "            input_ids = batch['input_ids'].to(model.device)\n",
    "            attention_mask = batch['attention_mask'].to(model.device)\n",
    "\n",
    "            # Generate summaries with the model\n",
    "            outputs = model.generate(input_ids, attention_mask=attention_mask, max_length=2048, num_beams=2)\n",
    "            batch_summaries = [tokenizer.decode(ids, skip_special_tokens=True) for ids in outputs]\n",
    "\n",
    "            # Append generated and actual summaries to the respective lists\n",
    "            summaries.extend(batch_summaries)\n",
    "            references.extend(batch['article_summary'])\n",
    "\n",
    "    return summaries, references\n",
    "\n",
    "# Generate summaries for the validation dataset\n",
    "generated_summaries, actual_summaries = generate_summaries(model, tokenizer, val_dataset, batch_size=8)\n",
    "\n",
    "# Compute and print the ROUGE score for evaluation\n",
    "rouge_score = rouge.compute(predictions=generated_summaries, references=actual_summaries)\n",
    "print(rouge_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge_scores = {\n",
    "    'rouge1': {\n",
    "        'low': {'precision': 0.5203, 'recall': 0.4547, 'fmeasure': 0.4632},\n",
    "        'mid': {'precision': 0.5354, 'recall': 0.4689, 'fmeasure': 0.4753},\n",
    "        'high': {'precision': 0.5502, 'recall': 0.4824, 'fmeasure': 0.4874}\n",
    "    },\n",
    "    'rouge2': {\n",
    "        'low': {'precision': 0.2507, 'recall': 0.2160, 'fmeasure': 0.2205},\n",
    "        'mid': {'precision': 0.2656, 'recall': 0.2292, 'fmeasure': 0.2331},\n",
    "        'high': {'precision': 0.2808, 'recall': 0.2428, 'fmeasure': 0.2459}\n",
    "    },\n",
    "    'rougeL': {\n",
    "        'low': {'precision': 0.4318, 'recall': 0.3784, 'fmeasure': 0.3843},\n",
    "        'mid': {'precision': 0.4465, 'recall': 0.3907, 'fmeasure': 0.3964},\n",
    "        'high': {'precision': 0.4613, 'recall': 0.4039, 'fmeasure': 0.4090}\n",
    "    },\n",
    "    'rougeLsum': {\n",
    "        'low': {'precision': 0.4324, 'recall': 0.3770, 'fmeasure': 0.3830},\n",
    "        'mid': {'precision': 0.4463, 'recall': 0.3903, 'fmeasure': 0.3960},\n",
    "        'high': {'precision': 0.4616, 'recall': 0.4031, 'fmeasure': 0.4075}\n",
    "    }\n",
    "}\n",
    "\n",
    "# Convert the nested dictionary into a Pandas DataFrame\n",
    "scores = pd.DataFrame.from_dict({(i, j): rouge_scores[i][j] \n",
    "                            for i in rouge_scores.keys() \n",
    "                            for j in rouge_scores[i].keys()},\n",
    "                            orient='index')\n",
    "\n",
    "# Set column names for readability\n",
    "scores.columns = ['Precision', 'Recall', 'F-Measure']\n",
    "\n",
    "# Display the DataFrame\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate a new summary with the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_text(text, max_length=5000):\n",
    "    \"\"\"\n",
    "    Generates a summary for the given text using a pre-trained model.\n",
    "\n",
    "    Args:\n",
    "        text (str): The text to be summarized.\n",
    "        max_length (int): The maximum length of the input text for the model.\n",
    "\n",
    "    Returns:\n",
    "        str: The generated summary of the input text.\n",
    "    \"\"\"\n",
    "    # Encode the input text using the tokenizer. The 'pt' indicates PyTorch tensors.\n",
    "    inputs = tokenizer.encode(text, return_tensors=\"pt\", max_length=max_length, truncation=False)\n",
    "    \n",
    "    # Move the encoded text to the same device as the model (e.g., GPU or CPU)\n",
    "    inputs = inputs.to(device)\n",
    "\n",
    "    # Generate summary IDs with the model. num_beams controls the beam search width.\n",
    "    # early_stopping is set to False for a thorough search, though it can be set to True for faster results.\n",
    "    summary_ids = model.generate(inputs, max_length=2000, num_beams=30, early_stopping=False)\n",
    "\n",
    "    # Decode the generated IDs back to text, skipping special tokens like padding or EOS.\n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    # Return the generated summary\n",
    "    return summary\n",
    "\n",
    "# Prompt the user to enter text for summarization\n",
    "text = input('Enter the text: ')\n",
    "print()\n",
    "\n",
    "# Call the summarize_text function to generate a summary of the input text\n",
    "summary = summarize_text(text)\n",
    "\n",
    "# Print the generated summary\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agentic Tri model approach\n",
    "1. **Initial Summary Generation**: Use a pre-trained language model to generate an initial summary of the input document.\n",
    "2. **Instruction Generation**: Develop a smaller model or rule-based system to generate editing instructions based on user preferences.\n",
    "3. **Summary Refinement**: Apply the editing instructions to the initial summary to produce the final personalized summary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForSeq2SeqLM, pipeline\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "\n",
    "# Check for GPU availability\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "print(f\"Running on device: {device}\")\n",
    "\n",
    "# Load the Mistral-7B model for generation and editing.\n",
    "# (Replace \"mistral-7b\" with the actual Hugging Face model identifier if needed.)\n",
    "mistral_model_name = \"mistral-7b\"  \n",
    "mistral_tokenizer = AutoTokenizer.from_pretrained(mistral_model_name)\n",
    "mistral_model = AutoModelForCausalLM.from_pretrained(mistral_model_name, torch_dtype=torch.float16)\n",
    "mistral_model.to(\"cuda\" if device==0 else \"cpu\")\n",
    "\n",
    "# Create a text-generation pipeline using Mistral-7B for summarization tasks.\n",
    "summarizer = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=mistral_model,\n",
    "    tokenizer=mistral_tokenizer,\n",
    "    device=device,\n",
    "    max_new_tokens=150,  # adjust as needed\n",
    ")\n",
    "\n",
    "# Load the Flan-T5-large model for the instructor.\n",
    "instructor_model_name = \"google/flan-t5-large\"\n",
    "instructor = pipeline(\n",
    "    \"text2text-generation\",\n",
    "    model=instructor_model_name,\n",
    "    tokenizer=instructor_model_name,\n",
    "    device=device,\n",
    "    max_new_tokens=50,\n",
    ")\n",
    "\n",
    "def generate_initial_summary(document: str) -> str:\n",
    "    \"\"\"\n",
    "    Uses the Mistral-7B model to generate an initial summary for the given document.\n",
    "    \"\"\"\n",
    "    prompt = f\"Summarize the following document concisely:\\n\\n{document}\\n\\nSummary:\"\n",
    "    generated = summarizer(prompt, do_sample=True, temperature=0.7)[0][\"generated_text\"]\n",
    "    # Post-process to extract the summary portion\n",
    "    # (Here we assume the generated text starts with the prompt and then the summary)\n",
    "    summary = generated.split(\"Summary:\")[-1].strip()\n",
    "    return summary\n",
    "\n",
    "def generate_instructions(document: str, initial_summary: str) -> str:\n",
    "    \"\"\"\n",
    "    Uses the instructor (Flan-T5-large) to generate editing instructions\n",
    "    to improve the initial summary based on the document.\n",
    "    \"\"\"\n",
    "    prompt = (\n",
    "        f\"Given the document and its initial summary, generate concise editing instructions \"\n",
    "        f\"to improve factual consistency and coverage.\\n\\n\"\n",
    "        f\"Document:\\n{document}\\n\\n\"\n",
    "        f\"Initial Summary:\\n{initial_summary}\\n\\n\"\n",
    "        f\"Editing Instructions:\"\n",
    "    )\n",
    "    instructions = instructor(prompt)[0][\"generated_text\"].strip()\n",
    "    return instructions\n",
    "\n",
    "def edit_summary(document: str, initial_summary: str, instructions: str) -> str:\n",
    "    \"\"\"\n",
    "    Uses the Mistral-7B model to produce an edited summary based on the document,\n",
    "    the initial summary, and the editing instructions.\n",
    "    \"\"\"\n",
    "    prompt = (\n",
    "        f\"Edit the initial summary based on the following instructions to better match user preferences. \\n\\n\"\n",
    "        f\"Document:\\n{document}\\n\\n\"\n",
    "        f\"Initial Summary:\\n{initial_summary}\\n\\n\"\n",
    "        f\"Instructions:\\n{instructions}\\n\\n\"\n",
    "        f\"Edited Summary:\"\n",
    "    )\n",
    "    edited = summarizer(prompt, do_sample=True, temperature=0.7)[0][\"generated_text\"]\n",
    "    edited_summary = edited.split(\"Edited Summary:\")[-1].strip()\n",
    "    return edited_summary\n",
    "\n",
    "def compute_reward(generated_summary: str, human_summary: str, alpha: float = 0.5, beta: float = 0.5) -> float:\n",
    "    \"\"\"\n",
    "    Computes a reward score based on both ROUGE and BLEU metrics.\n",
    "    \n",
    "    - ROUGE: Computes the average of ROUGE-1 and ROUGE-L F1 scores.\n",
    "    - BLEU: Uses the sentence BLEU score.\n",
    "    \n",
    "    The final reward is a weighted sum of both metrics (default weights are 0.5 each).\n",
    "    \"\"\"\n",
    "    # Initialize ROUGE scorer\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
    "    rouge_scores = scorer.score(human_summary, generated_summary)\n",
    "    \n",
    "    # Average F1 score for ROUGE-1 and ROUGE-L\n",
    "    rouge_f1 = (rouge_scores['rouge1'].fmeasure + rouge_scores['rougeL'].fmeasure) / 2.0\n",
    "    \n",
    "    # Compute BLEU score (tokenized by simple whitespace split)\n",
    "    reference_tokens = human_summary.split()\n",
    "    candidate_tokens = generated_summary.split()\n",
    "    bleu_score = sentence_bleu([reference_tokens], candidate_tokens)\n",
    "    \n",
    "    # Combine both metrics into a reward (you can adjust alpha and beta weights as needed)\n",
    "    reward = alpha * rouge_f1 + beta * bleu_score\n",
    "    return reward\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document = (\n",
    "    \"NASA's Artemis I mission has successfully launched, marking the first step towards \"\n",
    "    \"returning humans to the Moon. The unmanned mission tested key systems and demonstrated \"\n",
    "    \"the capabilities of the new Space Launch System (SLS) rocket. NASA officials say the mission \"\n",
    "    \"will pave the way for future manned missions, including plans for a lunar orbiting platform \"\n",
    "    \"and eventual crewed landings on the Moon.\"\n",
    ")\n",
    "\n",
    "# Generate the initial summary using the generator (Mistral-7B)\n",
    "initial_summary = generate_initial_summary(document)\n",
    "print(\"Initial Summary:\")\n",
    "print(initial_summary)\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# Generate editing instructions using the instructor (Flan-T5-large)\n",
    "instructions = generate_instructions(document, initial_summary)\n",
    "print(\"Editing Instructions:\")\n",
    "print(instructions)\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# Generate the edited (refined) summary using the editor (Mistral-7B)\n",
    "edited_summary = edit_summary(document, initial_summary, instructions)\n",
    "print(\"Edited Summary:\")\n",
    "print(edited_summary)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another example run using the excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "excel_file = \"summaries_of_articles.xlsx\"\n",
    "df = pd.read_excel(excel_file)\n",
    "\n",
    "# For demonstration, we'll process the first few rows\n",
    "results = []\n",
    "for index, row in df.iterrows():\n",
    "    document = row[\"article_content\"]\n",
    "    human_summary = row[\"human_written_summary\"]\n",
    "    \n",
    "    # Generate initial summary using Mistral-7B\n",
    "    init_summary = generate_initial_summary(document)\n",
    "    \n",
    "    # Generate editing instructions using Flan-T5-large\n",
    "    instructions = generate_instructions(document, init_summary)\n",
    "    \n",
    "    # Generate the edited summary using Mistral-7B\n",
    "    edited_summary = edit_summary(document, init_summary, instructions)\n",
    "    \n",
    "    # Compute reward comparing the edited summary with the human-written summary\n",
    "    reward = compute_reward(edited_summary, human_summary)\n",
    "    \n",
    "    results.append({\n",
    "        \"document\": document,\n",
    "        \"human_summary\": human_summary,\n",
    "        \"initial_summary\": init_summary,\n",
    "        \"edited_summary\": edited_summary,\n",
    "        \"reward\": reward\n",
    "    })\n",
    "    \n",
    "    # For demonstration, process only a few rows; remove break to process all rows.\n",
    "    if index >= 2:\n",
    "        break\n",
    "\n",
    "# Convert results to a DataFrame and display\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)\n",
    "\n",
    "# Optionally, save the results to an Excel file for later inspection.\n",
    "results_df.to_excel(\"summaries_with_rewards.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deepseek code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "class OfflineMistralSummarizer:\n",
    "    def __init__(self, model_path=\"mistral-7b-instruct-v0.1.Q5_K_M.gguf\"):\n",
    "        # Initialize Mistral 7B GGUF model (download from Hugging Face)\n",
    "        self.llm = Llama(\n",
    "            model_path=model_path,\n",
    "            n_ctx=4096,  # Context window for long documents\n",
    "            n_gpu_layers=40,  # Offload to GPU if available\n",
    "            n_threads=8  # CPU threads\n",
    "        )\n",
    "        \n",
    "        # Initialize local instructor model (small Flan-T5 for comparison)\n",
    "        self.instructor_tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-base\")\n",
    "        self.instructor_model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-base\")\n",
    "\n",
    "    def generate_initial_summary(self, document):\n",
    "        \"\"\"Generator Agent: Mistral 7B produces initial summary\"\"\"\n",
    "        prompt = f\"\"\"<s>[INST] You are an AI summarizer. Create a concise summary of this document:\n",
    "        {document}\n",
    "        [/INST] Summary:\"\"\"\n",
    "        \n",
    "        response = self.llm(\n",
    "            prompt=prompt,\n",
    "            max_tokens=512,\n",
    "            temperature=0.7,\n",
    "            stop=[\"</s>\"]\n",
    "        )\n",
    "        return response['choices'][0]['text'].strip()\n",
    "\n",
    "    def generate_instructions(self, document, initial_summary, user_preferences):\n",
    "        \"\"\"Instructor Agent: Generate editing instructions using local model\"\"\"\n",
    "        input_text = f\"Document: {document}\\nSummary: {initial_summary}\\nPreferences: {user_preferences}\"\n",
    "        input_ids = self.instructor_tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
    "        \n",
    "        outputs = self.instructor_model.generate(input_ids)\n",
    "        return self.instructor_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    def edit_summary(self, document, initial_summary, instructions):\n",
    "        \"\"\"Editor Agent: Mistral 7B refines summary with instructions\"\"\"\n",
    "        prompt = f\"\"\"<s>[INST] Revise this summary based on the user's preferences:\n",
    "            Document: {document}\n",
    "            Original Summary: {initial_summary}\n",
    "            Editing Instructions: {instructions}\n",
    "            [/INST] Revised Summary:\"\"\"\n",
    "        \n",
    "        response = self.llm(\n",
    "            prompt=prompt,\n",
    "            max_tokens=1024,\n",
    "            temperature=0.5,\n",
    "            stop=[\"</s>\"]\n",
    "        )\n",
    "        return response['choices'][0]['text'].strip()\n",
    "\n",
    "    def calculate_reward(self, original_summary, edited_summary, user_preferences):\n",
    "        \"\"\"Simplified reward calculation (replace with your metrics)\"\"\"\n",
    "        # Overlap score\n",
    "        overlap = len(set(original_summary.split()) & set(edited_summary.split())) / len(set(original_summary.split()))\n",
    "        \n",
    "        # Preference score\n",
    "        pref_score = sum(1 for kw in user_preferences if kw.lower() in edited_summary.lower())\n",
    "        \n",
    "        return overlap + pref_score\n",
    "\n",
    "# Usage Example\n",
    "if __name__ == \"__main__\":\n",
    "    # First download the GGUF model from Hugging Face:\n",
    "    # wget https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF/resolve/main/mistral-7b-instruct-v0.1.Q5_K_M.gguf\n",
    "    \n",
    "    summarizer = OfflineMistralSummarizer(model_path=\"./mistral-7b-instruct-v0.1.Q5_K_M.gguf\")\n",
    "    \n",
    "    document = \"\"\"Machine learning (ML) is transforming industries through... [long text]...\"\"\"\n",
    "    user_prefs = [\"technical details\", \"model architectures\", \"performance metrics\"]\n",
    "    \n",
    "    # Generation pipeline\n",
    "    print(\"Generating initial summary...\")\n",
    "    initial = summarizer.generate_initial_summary(document)\n",
    "    print(\"Creating instructions...\")\n",
    "    instructions = summarizer.generate_instructions(document, initial, user_prefs)\n",
    "    print(\"Editing summary...\")\n",
    "    final = summarizer.edit_summary(document, initial, instructions)\n",
    "    \n",
    "    print(f\"\\nInitial Summary: {initial}\")\n",
    "    print(f\"\\nInstructions: {instructions}\")\n",
    "    print(f\"\\nFinal Summary: {final}\")\n",
    "    print(f\"\\nReward Score: {summarizer.calculate_reward(initial, final, user_prefs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chain of Thought approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load spaCy model for Named Entity Recognition (NER)\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Load a pre-trained summarization model\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "\n",
    "def extract_elements(text):\n",
    "    \"\"\"\n",
    "    Extract key elements from the text using Named Entity Recognition (NER).\n",
    "    \"\"\"\n",
    "    doc = nlp(text)\n",
    "    elements = {\n",
    "        \"entities\": [],\n",
    "        \"dates\": [],\n",
    "        \"events\": [],\n",
    "        \"locations\": []\n",
    "    }\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ in [\"PERSON\", \"ORG\"]:\n",
    "            elements[\"entities\"].append(ent.text)\n",
    "        elif ent.label_ == \"DATE\":\n",
    "            elements[\"dates\"].append(ent.text)\n",
    "        elif ent.label_ == \"GPE\":\n",
    "            elements[\"locations\"].append(ent.text)\n",
    "        # Events are not directly recognized by spaCy's NER,\n",
    "        # so we may need additional processing or a custom model.\n",
    "    return elements\n",
    "\n",
    "def generate_summary(text, elements):\n",
    "    \"\"\"\n",
    "    Generate a summary by incorporating extracted elements.\n",
    "    \"\"\"\n",
    "    # Initial summary generation\n",
    "    initial_summary = summarizer(text, max_length=150, min_length=40, do_sample=False)[0]['summary_text']\n",
    "    \n",
    "    # Incorporate extracted elements into the summary\n",
    "    # This is a simplified approach; more sophisticated methods can be applied.\n",
    "    summary = initial_summary\n",
    "    if elements[\"entities\"]:\n",
    "        summary += f\" Key entities involved: {', '.join(set(elements['entities']))}.\"\n",
    "    if elements[\"dates\"]:\n",
    "        summary += f\" Relevant dates: {', '.join(set(elements['dates']))}.\"\n",
    "    if elements[\"locations\"]:\n",
    "        summary += f\" Locations mentioned: {', '.join(set(elements['locations']))}.\"\n",
    "    # Events would be added here if extracted\n",
    "    \n",
    "    return summary\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    article = \"\"\"\n",
    "    The 69-year-old's Yamaha collided with a Nissan car between Handley's Corner and Barregarrow crossroads at about 17:00 BST on 4 June. Mr. Baker, who was from the island, was airlifted to Noble's Hospital, where he later died. The car driver, who police say was Northern Irish, was treated in hospital but has been discharged. Another motorcyclist who was injured after the crash has also been released from hospital.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Step 1: Extract elements\n",
    "    elements = extract_elements(article)\n",
    "    print(\"Extracted Elements:\", elements)\n",
    "    \n",
    "    # Step 2: Generate summary\n",
    "    summary = generate_summary(article, elements)\n",
    "    print(\"Generated Summary:\", summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine Tuning Tests (QLoRA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" ##specify the gpu you want to use\n",
    "\n",
    "import logging\n",
    "import torch\n",
    "from trl import SFTTrainer\n",
    "from datasets import Dataset, DatasetDict, load_metric, load_dataset\n",
    "from peft import LoraConfig, AutoPeftModelForCausalLM, prepare_model_for_kbit_training, get_peft_model, PeftModel\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments, Trainer, AutoConfig\n",
    "\n",
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "        Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params, all_param = 0, 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    logger.info(f\"Trainable params: {trainable_params} || All params: {all_param} || Trainable (%): {100 * trainable_params / all_param}\")\n",
    "\n",
    "def load_model(model_path):\n",
    "    model_name = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "\n",
    "    if not model_path:\n",
    "        model_path = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "    \n",
    "    compute_dtype = getattr(torch, \"float16\")\n",
    "    bnb_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_use_double_quant=False, bnb_4bit_quant_type=\"nf4\",\n",
    "                                    bnb_4bit_compute_dtype=compute_dtype)\n",
    "\n",
    "    # Check GPU compatibility with bfloat16\n",
    "    if compute_dtype == torch.float16:\n",
    "        major, _ = torch.cuda.get_device_capability()\n",
    "        if major >= 8:\n",
    "            logger.info(\"=\" * 80)\n",
    "            logger.info(\"GPU supports bfloat16: accelerate training with bf16=True\")\n",
    "            logger.info(\"=\" * 80)    \n",
    "\n",
    "    \n",
    "    # load model and tokenizer\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_path, quantization_config=bnb_config, device_map=\"auto\",\n",
    "                                                 trust_remote_code=True, attn_implementation=\"flash_attention_2\")\n",
    "    model.config.use_cache = False\n",
    "    model.config.pretraining_tp = 1\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, model_max_length=4096, add_eos_token=True)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    # Fix weird overflow issue with fp16 training\n",
    "    tokenizer.padding_side = \"right\" \n",
    "\n",
    "    # lora config file\n",
    "    config = LoraConfig(\n",
    "      r=32, # Sets the rank of the LoRA adaptation, which controls the size of low-rank updates.\n",
    "      lora_alpha=16, # Scaling factor for the LoRA weights.\n",
    "      bias=\"none\", \n",
    "      lora_dropout=0.1, \n",
    "      target_modules=[ # Specifies the layers to which LoRA will be applied (e.g., projection layers in the attention and feedforward network).\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\", \"lm_head\"\n",
    "      ],\n",
    "      task_type=\"CAUSAL_LM\"\n",
    "    )\n",
    "\n",
    "    model = get_peft_model(model, config)\n",
    "    print_trainable_parameters(model)\n",
    "\n",
    "    # Apply the accelerator.\n",
    "    # model = accelerator.prepare_model(model)\n",
    "\n",
    "    model.gradient_checkpointing_enable()\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "    # Load LoRA configuration\n",
    "    peft_config = LoraConfig(r=8, lora_alpha=16, lora_dropout=0.1,  bias=\"none\", task_type=\"CAUSAL_LM\")\n",
    "    \n",
    "    return model, tokenizer, peft_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing QLoRA for fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "def train_args(output_dir, dataset, model_path=None):\n",
    "    \n",
    "    logger.info(f\"Loading model ....\")\n",
    "    model, tokenizer, peft_config = load_model(model_path)\n",
    "    \n",
    "    training_arguments = TrainingArguments(\n",
    "    output_dir=output_dir, \n",
    "    num_train_epochs=5, \n",
    "    per_device_train_batch_size=1, # batch size for training is 1 per GPU\n",
    "    gradient_accumulation_steps=2, # gradients will be accumulated over 2 steps\n",
    "    gradient_checkpointing = True, \n",
    "    save_steps=1000, # save model every {save_steps} steps\n",
    "    logging_steps=200, \n",
    "    max_steps=-1, # -1 indicates \"no step limit\"\n",
    "    optim=\"paged_adamw_32bit\", # Uses the 32-bit Paged AdamW optimizer, an optimized version of the AdamW optimizer, suited for training in lower precision\n",
    "    learning_rate=2e-4, \n",
    "    weight_decay=0.001, \n",
    "    fp16=False, bf16=True, # Training is done in bfloat16 precision (bf16=True), which is more stable than FP16 in terms of numerical accuracy but still provides the speed and memory efficiency advantages. \n",
    "    max_grad_norm=0.3, # limit grad's norm to 0.3 to prevent exploding gradient\n",
    "    warmup_ratio=0.03, #  Specifies a warmup period where the learning rate gradually increases during the first {warmup_ratio}% of training\n",
    "    group_by_length=True, \n",
    "    lr_scheduler_type=\"cosine\" # Uses a cosine learning rate scheduler, which reduces the learning rate following a cosine curve after the warmup phase.\n",
    "    )\n",
    "      \n",
    "    # Pack multiple short examples in the same input sequence to increase efficiency\n",
    "    packing = False\n",
    "    \n",
    "    # Set supervised fine-tuning parameters\n",
    "    trainer = SFTTrainer(\n",
    "    model=model, \n",
    "    train_dataset=dataset, \n",
    "    peft_config=peft_config, \n",
    "    dataset_text_field=\"text\", # Specifies that the text data is contained in a field named \"text\" in the dataset.\n",
    "    max_seq_length=4096, # Sets the maximum sequence length to 4096 tokens, which is the upper limit for Mistral models.\n",
    "    tokenizer=tokenizer, \n",
    "    args=training_arguments, \n",
    "    packing=False # Disables packing, meaning that sequences will not be combined during training.\n",
    "    )\n",
    "\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call functions and train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"./models/\"\n",
    "dataset = get_dataset('training_data.csv')\n",
    "model_path = None\n",
    "\n",
    "trainer = train_args(output_dir, dataset, model_path)\n",
    "\n",
    "# Train model\n",
    "trainer.train()\n",
    "\n",
    "# Save trained model\n",
    "output_dir = os.path.join(output_dir, \"final_ckpt\")\n",
    "trainer.model.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the fine tuned mistral model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(ckpt_path, model_name):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "    tokenizer.pad_token = tokenizer.unk_token\n",
    "    tokenizer.padding_side = \"right\"  \n",
    "    \n",
    "    streamer = TextIteratorStreamer(tokenizer, skip_prompt=True, timeout=20.0, skip_special_tokens=True)\n",
    "    model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "      ckpt_path, \n",
    "      low_cpu_mem_usage=True, \n",
    "      torch_dtype=torch.bfloat16,\n",
    "      load_in_4bit=True\n",
    "    )\n",
    "\n",
    "    logger.info(f\"Model loaded successfully\")\n",
    "    return model, tokenizer, streamer\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "ckpt_path = './nlp_exp/mistral_ckpts/checkpoint-20000'\n",
    "\n",
    "model, tokenizer, streamer = load_model(ckpt_path, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference(prompts, model, tokenizer):\n",
    "    model_inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True, max_length=16384).to(\"cuda\")\n",
    "    print(f\"Number of tokens in the input string: {model_inputs['input_ids'].shape}\")\n",
    "    \n",
    "    with torch.inference_mode():\n",
    "        outputs = model.generate(\n",
    "            **model_inputs, # The tokenized input prompts are passed to the model for generation.\n",
    "            max_new_tokens=1250, #  The model will generate up to 1250 new tokens. This sets the upper limit for the length of the generated text.\n",
    "            repetition_penalty=1.2, # This parameter penalizes repeated tokens during generation, encouraging the model to avoid repetitive text.\n",
    "            pad_token_id=tokenizer.pad_token_id \n",
    "        )\n",
    "    \n",
    "    final_text = tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0][len(prompts):]\n",
    "    return final_text.strip()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

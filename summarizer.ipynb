{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\ianwa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error while downloading from https://cdn-lfs.hf.co/facebook/bart-large-cnn/40041830399afb5348525ef8354b007ecec4286fdf3524f7e6b54377e17096cb?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27model.safetensors%3B+filename%3D%22model.safetensors%22%3B&Expires=1739501882&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTczOTUwMTg4Mn19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5oZi5jby9mYWNlYm9vay9iYXJ0LWxhcmdlLWNubi80MDA0MTgzMDM5OWFmYjUzNDg1MjVlZjgzNTRiMDA3ZWNlYzQyODZmZGYzNTI0ZjdlNmI1NDM3N2UxNzA5NmNiP3Jlc3BvbnNlLWNvbnRlbnQtZGlzcG9zaXRpb249KiJ9XX0_&Signature=KIkV7LAM8tGgmRPypOohGxPI1ezFPvMCHZxUtPI3Xwi1D7kQDfVK1E358jJHBwmo5mEvzYRxVTB7ZQwU0qqrSSDU2YtzaWnDxKFLWjwc4A1zhWvOIE85yVQdRE7t6E86k1cwSzjK%7EtthdCbkpPx0liyLOW-iqH-1GPOzzQ06t0VuuiFWZeUGVGeVMdmUHSwrYMetpRPbCnnv70mo8Pi87RRPM222f5BXCQ0Mknty5xgCmp-GdZDSdS-HH9fg6cfK42b24EyRfbOEx8OFtYmyrxuVtppV%7E1RIjezUtuNMyEVhdj7NXfFsFnTuF61dzMKat6REBPlcSJHA71M30awKDw__&Key-Pair-Id=K3RPWS32NSSJCE: HTTPSConnectionPool(host='cdn-lfs.hf.co', port=443): Read timed out.\n",
      "Trying to resume download...\n",
      "c:\\Users\\ianwa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\ianwa\\.cache\\huggingface\\hub\\models--facebook--bart-large-cnn. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\ianwa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBartForConditionalGeneration.\n",
      "\n",
      "All the weights of TFBartForConditionalGeneration were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBartForConditionalGeneration for predictions without further training.\n",
      "Device set to use 0\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "\n",
    "ARTICLE = \"\"\" New York (CNN)When Liana Barrientos was 23 years old, she got married in Westchester County, New York.\n",
    "A year later, she got married again in Westchester County, but to a different man and without divorcing her first husband.\n",
    "Only 18 days after that marriage, she got hitched yet again. Then, Barrientos declared \"I do\" five more times, sometimes only within two weeks of each other.\n",
    "In 2010, she married once more, this time in the Bronx. In an application for a marriage license, she stated it was her \"first and only\" marriage.\n",
    "Barrientos, now 39, is facing two criminal counts of \"offering a false instrument for filing in the first degree,\" referring to her false statements on the\n",
    "2010 marriage license application, according to court documents.\n",
    "Prosecutors said the marriages were part of an immigration scam.\n",
    "On Friday, she pleaded not guilty at State Supreme Court in the Bronx, according to her attorney, Christopher Wright, who declined to comment further.\n",
    "After leaving court, Barrientos was arrested and charged with theft of service and criminal trespass for allegedly sneaking into the New York subway through an emergency exit, said Detective\n",
    "Annette Markowski, a police spokeswoman. In total, Barrientos has been married 10 times, with nine of her marriages occurring between 1999 and 2002.\n",
    "All occurred either in Westchester County, Long Island, New Jersey or the Bronx. She is believed to still be married to four men, and at one time, she was married to eight men at once, prosecutors say.\n",
    "Prosecutors said the immigration scam involved some of her husbands, who filed for permanent residence status shortly after the marriages.\n",
    "Any divorces happened only after such filings were approved. It was unclear whether any of the men will be prosecuted.\n",
    "The case was referred to the Bronx District Attorney\\'s Office by Immigration and Customs Enforcement and the Department of Homeland Security\\'s\n",
    "Investigation Division. Seven of the men are from so-called \"red-flagged\" countries, including Egypt, Turkey, Georgia, Pakistan and Mali.\n",
    "Her eighth husband, Rashid Rajput, was deported in 2006 to his native Pakistan after an investigation by the Joint Terrorism Task Force.\n",
    "If convicted, Barrientos faces up to four years in prison.  Her next court appearance is scheduled for May 18.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'summary_text': 'Liana Barrientos, 39, is charged with two counts of \"offering a false instrument for filing in the first degree\" In total, she has been married 10 times, with nine of her marriages occurring between 1999 and 2002. She is believed to still be married to four men.'}]\n"
     ]
    }
   ],
   "source": [
    "print(summarizer(ARTICLE, max_length=130, min_length=30, do_sample=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying out new code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ianwa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"argilla/news-summary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ianwa\\AppData\\Local\\Temp\\ipykernel_9704\\2027293581.py:6: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  df_train['prediction'][i] = df_train['prediction'][i][0]['text']\n",
      "C:\\Users\\ianwa\\AppData\\Local\\Temp\\ipykernel_9704\\2027293581.py:11: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  df_test['prediction'][i] = df_test['prediction'][i][0]['text']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_train = pd.DataFrame(ds['test'])\n",
    "df_train = df_train.drop(columns=[ 'prediction_agent', 'annotation', 'annotation_agent', 'id' ,'metadata', 'status', 'event_timestamp', 'metrics'])\n",
    "for i in range(len(df_train)):\n",
    "    df_train['prediction'][i] = df_train['prediction'][i][0]['text']\n",
    "\n",
    "df_test = pd.DataFrame(ds['train'])\n",
    "df_test = df_test.drop(columns=[ 'prediction_agent', 'annotation', 'annotation_agent', 'id' ,'metadata', 'status', 'event_timestamp', 'metrics'])\n",
    "for i in range(len(df_test)):\n",
    "    df_test['prediction'][i] = df_test['prediction'][i][0]['text']\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_train, df_val = train_test_split(df_train, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of df train: 16333\n",
      "len of df validation: 4084\n",
      "len of df test: 1000\n"
     ]
    }
   ],
   "source": [
    "print('len of df train:', len(df_train))\n",
    "print('len of df validation:', len(df_val))\n",
    "print('len of df test:', len(df_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample of df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_content</th>\n",
       "      <th>article_summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14764</th>\n",
       "      <td>BERLIN (Reuters) - The German military s procu...</td>\n",
       "      <td>Report shows 1,300 unfilled jobs, strain for G...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7537</th>\n",
       "      <td>WASHINGTON (Reuters) - U.S.-backed militias fi...</td>\n",
       "      <td>U.S.-backed forces not planning on entering De...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16178</th>\n",
       "      <td>SOCHI, Russia (Reuters) - Russian President Vl...</td>\n",
       "      <td>Putin says Russia will respond if Russian medi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6161</th>\n",
       "      <td>JACKSON, Ga. (Reuters) - Down a Georgia count...</td>\n",
       "      <td>U.S. militia girds for trouble as presidential...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1090</th>\n",
       "      <td>SEOUL (Reuters) - The escalating threat arisin...</td>\n",
       "      <td>North Korea missile crisis seen pushing South ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11284</th>\n",
       "      <td>HONOLULU (Reuters) - A federal judge in Hawaii...</td>\n",
       "      <td>Federal judge in Hawaii extends court order bl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11964</th>\n",
       "      <td>VIENNA (Reuters) - Austria s conservative Peop...</td>\n",
       "      <td>Austria's conservatives reach coalition deal w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5390</th>\n",
       "      <td>KANDAHAR, Afghanistan (Reuters) - A roadside b...</td>\n",
       "      <td>Six civilians killed by roadside bomb in Afgha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>860</th>\n",
       "      <td>ROME (Reuters) - The Italian Chamber of Deputi...</td>\n",
       "      <td>Italy lower house passes new electoral law, mo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15795</th>\n",
       "      <td>WASHINGTON (Reuters) - With no palatable milit...</td>\n",
       "      <td>Trump seeks tougher sanctions to prod North Ko...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16333 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         article_content  \\\n",
       "14764  BERLIN (Reuters) - The German military s procu...   \n",
       "7537   WASHINGTON (Reuters) - U.S.-backed militias fi...   \n",
       "16178  SOCHI, Russia (Reuters) - Russian President Vl...   \n",
       "6161    JACKSON, Ga. (Reuters) - Down a Georgia count...   \n",
       "1090   SEOUL (Reuters) - The escalating threat arisin...   \n",
       "...                                                  ...   \n",
       "11284  HONOLULU (Reuters) - A federal judge in Hawaii...   \n",
       "11964  VIENNA (Reuters) - Austria s conservative Peop...   \n",
       "5390   KANDAHAR, Afghanistan (Reuters) - A roadside b...   \n",
       "860    ROME (Reuters) - The Italian Chamber of Deputi...   \n",
       "15795  WASHINGTON (Reuters) - With no palatable milit...   \n",
       "\n",
       "                                         article_summary  \n",
       "14764  Report shows 1,300 unfilled jobs, strain for G...  \n",
       "7537   U.S.-backed forces not planning on entering De...  \n",
       "16178  Putin says Russia will respond if Russian medi...  \n",
       "6161   U.S. militia girds for trouble as presidential...  \n",
       "1090   North Korea missile crisis seen pushing South ...  \n",
       "...                                                  ...  \n",
       "11284  Federal judge in Hawaii extends court order bl...  \n",
       "11964  Austria's conservatives reach coalition deal w...  \n",
       "5390   Six civilians killed by roadside bomb in Afgha...  \n",
       "860    Italy lower house passes new electoral law, mo...  \n",
       "15795  Trump seeks tougher sanctions to prod North Ko...  \n",
       "\n",
       "[16333 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = df_train.rename(columns={\"text\": \"article_content\", \"prediction\": \"article_summary\"})\n",
    "df_test = df_test.rename(columns={\"text\": \"article_content\", \"prediction\": \"article_summary\"})\n",
    "df_val = df_val.rename(columns={\"text\": \"article_content\", \"prediction\": \"article_summary\"})\n",
    "df_train\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine tuned BART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class SummaryDataset(Dataset):\n",
    "    # Initialize the dataset with a tokenizer, data, and maximum token length\n",
    "    def __init__(self, tokenizer, data, max_length=512):\n",
    "        self.tokenizer = tokenizer  # Tokenizer for encoding text\n",
    "        self.data = data            # Data containing dialogues and summaries\n",
    "        self.max_length = max_length # Maximum length of tokens\n",
    "\n",
    "    # Return the number of items in the dataset\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    # Retrieve an item from the dataset by index\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data.iloc[idx]  # Get the row at the specified index\n",
    "        dialogue = item['article_content'] # Extract dialogue from the row\n",
    "        summary = item['article_summary']   # Extract summary from the row\n",
    "\n",
    "        # Encode the dialogue as input data for the model\n",
    "        source = self.tokenizer.encode_plus(\n",
    "            dialogue, \n",
    "            max_length=self.max_length, \n",
    "            padding='max_length', \n",
    "            return_tensors='pt', \n",
    "            truncation=True\n",
    "        )\n",
    "\n",
    "        # Encode the summary as target data for the model\n",
    "        target = self.tokenizer.encode_plus(\n",
    "            summary, \n",
    "            max_length=self.max_length, \n",
    "            padding='max_length', \n",
    "            return_tensors='pt', \n",
    "            truncation=True\n",
    "        )\n",
    "\n",
    "        # Return a dictionary containing input_ids, attention_mask, labels, and the original summary text\n",
    "        return {\n",
    "            'input_ids': source['input_ids'].flatten(),\n",
    "            'attention_mask': source['attention_mask'].flatten(),\n",
    "            'labels': target['input_ids'].flatten(),\n",
    "            'summary': summary \n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "\n",
    "# Initialize the tokenizer for BART\n",
    "# 'facebook/bart-base' is a pretrained model identifier\n",
    "# The tokenizer is responsible for converting text input into tokens that the model can understand\n",
    "tokenizer = BartTokenizer.from_pretrained('facebook/bart-base')\n",
    "\n",
    "# Initialize the BART model for conditional generation\n",
    "# This model is used for tasks like summarization where the output is conditional on the input text\n",
    "# The model is loaded with pretrained weights from 'facebook/bart-base'\n",
    "model = BartForConditionalGeneration.from_pretrained('facebook/bart-base')\n",
    "\n",
    "# Creating an instance of the SummaryDataset class for training data\n",
    "# It uses the tokenizer to process the training data (train_df) \n",
    "# for model input\n",
    "train_dataset = SummaryDataset(tokenizer, df_train)\n",
    "\n",
    "# Creating an instance of the SummaryDataset class for validation data\n",
    "# It uses the same tokenizer but with a different dataset (test_df2) \n",
    "# for validation purposes\n",
    "val_dataset = SummaryDataset(tokenizer, df_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\ianwa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ianwa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='23' max='4084' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  23/4084 38:23 < 123:42:52, 0.01 it/s, Epoch 0.01/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 26\u001b[0m\n\u001b[0;32m     18\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[0;32m     19\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,             \u001b[38;5;66;03m# The model to be trained (e.g., our BART model)\u001b[39;00m\n\u001b[0;32m     20\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,      \u001b[38;5;66;03m# Training arguments specifying training parameters like learning rate, batch size, etc.\u001b[39;00m\n\u001b[0;32m     21\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mtrain_dataset,  \u001b[38;5;66;03m# The dataset to be used for training the model\u001b[39;00m\n\u001b[0;32m     22\u001b[0m     eval_dataset\u001b[38;5;241m=\u001b[39mval_dataset    \u001b[38;5;66;03m# The dataset to be used for evaluating the model during training\u001b[39;00m\n\u001b[0;32m     23\u001b[0m )\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Starting the training process\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ianwa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\trainer.py:2171\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2169\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2170\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2171\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2172\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2175\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2176\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ianwa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\trainer.py:2531\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2524\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   2525\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[0;32m   2526\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   2527\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m!=\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED\n\u001b[0;32m   2528\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[0;32m   2529\u001b[0m )\n\u001b[0;32m   2530\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[1;32m-> 2531\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2533\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2534\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   2535\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m   2536\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   2537\u001b[0m ):\n\u001b[0;32m   2538\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   2539\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32mc:\\Users\\ianwa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\trainer.py:3675\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[0;32m   3672\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m   3674\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[1;32m-> 3675\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3677\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[0;32m   3678\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   3679\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   3680\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m   3681\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\ianwa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\trainer.py:3731\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[1;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[0;32m   3729\u001b[0m         loss_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_items_in_batch\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m num_items_in_batch\n\u001b[0;32m   3730\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mloss_kwargs}\n\u001b[1;32m-> 3731\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3732\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[0;32m   3733\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[0;32m   3734\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\ianwa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ianwa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\ianwa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\models\\bart\\modeling_bart.py:1659\u001b[0m, in \u001b[0;36mBartForConditionalGeneration.forward\u001b[1;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1637\u001b[0m         decoder_input_ids \u001b[38;5;241m=\u001b[39m shift_tokens_right(\n\u001b[0;32m   1638\u001b[0m             labels, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpad_token_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mdecoder_start_token_id\n\u001b[0;32m   1639\u001b[0m         )\n\u001b[0;32m   1641\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(\n\u001b[0;32m   1642\u001b[0m     input_ids,\n\u001b[0;32m   1643\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1656\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[0;32m   1657\u001b[0m )\n\u001b[1;32m-> 1659\u001b[0m lm_logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlm_head\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1660\u001b[0m lm_logits \u001b[38;5;241m=\u001b[39m lm_logits \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinal_logits_bias\u001b[38;5;241m.\u001b[39mto(lm_logits\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m   1662\u001b[0m masked_lm_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ianwa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ianwa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\ianwa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "# Define training arguments for the model\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # Directory to save model output and checkpoints\n",
    "    num_train_epochs=2,              # Number of epochs to train the model\n",
    "    per_device_train_batch_size=8,   # Batch size per device during training\n",
    "    per_device_eval_batch_size=8,    # Batch size for evaluation\n",
    "    warmup_steps=500,                # Number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # Weight decay for regularization\n",
    "    logging_dir='./logs',            # Directory to save logs\n",
    "    logging_steps=10,                # Log metrics every specified number of steps\n",
    "    evaluation_strategy=\"epoch\",     # Evaluation is done at the end of each epoch\n",
    "    report_to='none'                 # Disables reporting to any online services (e.g., TensorBoard, WandB)\n",
    ")\n",
    "\n",
    "# Initializing the Trainer object\n",
    "trainer = Trainer(\n",
    "    model=model,             # The model to be trained (e.g., our BART model)\n",
    "    args=training_args,      # Training arguments specifying training parameters like learning rate, batch size, etc.\n",
    "    train_dataset=train_dataset,  # The dataset to be used for training the model\n",
    "    eval_dataset=val_dataset    # The dataset to be used for evaluating the model during training\n",
    ")\n",
    "\n",
    "# Starting the training process\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation portion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "\n",
    "# Load the ROUGE metric for evaluation\n",
    "rouge = load_metric('rouge')\n",
    "\n",
    "def generate_summaries(model, tokenizer, dataset, batch_size=8):\n",
    "    \"\"\"\n",
    "    Generate summaries using the provided model and tokenizer on the given dataset.\n",
    "\n",
    "    Args:\n",
    "        model: The trained summarization model.\n",
    "        tokenizer: Tokenizer associated with the model.\n",
    "        dataset: Dataset for which summaries need to be generated.\n",
    "        batch_size: Number of data samples to process in each batch.\n",
    "\n",
    "    Returns:\n",
    "        summaries: Generated summaries by the model.\n",
    "        references: Actual summaries from the dataset for comparison.\n",
    "    \"\"\"\n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "    summaries = []    # List to store generated summaries\n",
    "    references = []   # List to store actual summaries\n",
    "\n",
    "    # Create a DataLoader for batch processing\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size)\n",
    "\n",
    "    # Disabled gradient calculations for efficiency\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            # Move input data to the same device as the model\n",
    "            input_ids = batch['input_ids'].to(model.device)\n",
    "            attention_mask = batch['attention_mask'].to(model.device)\n",
    "\n",
    "            # Generate summaries with the model\n",
    "            outputs = model.generate(input_ids, attention_mask=attention_mask, max_length=2048, num_beams=2)\n",
    "            batch_summaries = [tokenizer.decode(ids, skip_special_tokens=True) for ids in outputs]\n",
    "\n",
    "            # Append generated and actual summaries to the respective lists\n",
    "            summaries.extend(batch_summaries)\n",
    "            references.extend(batch['article_summary'])\n",
    "\n",
    "    return summaries, references\n",
    "\n",
    "# Generate summaries for the validation dataset\n",
    "generated_summaries, actual_summaries = generate_summaries(model, tokenizer, val_dataset, batch_size=8)\n",
    "\n",
    "# Compute and print the ROUGE score for evaluation\n",
    "rouge_score = rouge.compute(predictions=generated_summaries, references=actual_summaries)\n",
    "print(rouge_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge_scores = {\n",
    "    'rouge1': {\n",
    "        'low': {'precision': 0.5203, 'recall': 0.4547, 'fmeasure': 0.4632},\n",
    "        'mid': {'precision': 0.5354, 'recall': 0.4689, 'fmeasure': 0.4753},\n",
    "        'high': {'precision': 0.5502, 'recall': 0.4824, 'fmeasure': 0.4874}\n",
    "    },\n",
    "    'rouge2': {\n",
    "        'low': {'precision': 0.2507, 'recall': 0.2160, 'fmeasure': 0.2205},\n",
    "        'mid': {'precision': 0.2656, 'recall': 0.2292, 'fmeasure': 0.2331},\n",
    "        'high': {'precision': 0.2808, 'recall': 0.2428, 'fmeasure': 0.2459}\n",
    "    },\n",
    "    'rougeL': {\n",
    "        'low': {'precision': 0.4318, 'recall': 0.3784, 'fmeasure': 0.3843},\n",
    "        'mid': {'precision': 0.4465, 'recall': 0.3907, 'fmeasure': 0.3964},\n",
    "        'high': {'precision': 0.4613, 'recall': 0.4039, 'fmeasure': 0.4090}\n",
    "    },\n",
    "    'rougeLsum': {\n",
    "        'low': {'precision': 0.4324, 'recall': 0.3770, 'fmeasure': 0.3830},\n",
    "        'mid': {'precision': 0.4463, 'recall': 0.3903, 'fmeasure': 0.3960},\n",
    "        'high': {'precision': 0.4616, 'recall': 0.4031, 'fmeasure': 0.4075}\n",
    "    }\n",
    "}\n",
    "\n",
    "# Convert the nested dictionary into a Pandas DataFrame\n",
    "scores = pd.DataFrame.from_dict({(i, j): rouge_scores[i][j] \n",
    "                            for i in rouge_scores.keys() \n",
    "                            for j in rouge_scores[i].keys()},\n",
    "                            orient='index')\n",
    "\n",
    "# Set column names for readability\n",
    "scores.columns = ['Precision', 'Recall', 'F-Measure']\n",
    "\n",
    "# Display the DataFrame\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate a new summary with the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_text(text, max_length=5000):\n",
    "    \"\"\"\n",
    "    Generates a summary for the given text using a pre-trained model.\n",
    "\n",
    "    Args:\n",
    "        text (str): The text to be summarized.\n",
    "        max_length (int): The maximum length of the input text for the model.\n",
    "\n",
    "    Returns:\n",
    "        str: The generated summary of the input text.\n",
    "    \"\"\"\n",
    "    # Encode the input text using the tokenizer. The 'pt' indicates PyTorch tensors.\n",
    "    inputs = tokenizer.encode(text, return_tensors=\"pt\", max_length=max_length, truncation=False)\n",
    "    \n",
    "    # Move the encoded text to the same device as the model (e.g., GPU or CPU)\n",
    "    inputs = inputs.to(device)\n",
    "\n",
    "    # Generate summary IDs with the model. num_beams controls the beam search width.\n",
    "    # early_stopping is set to False for a thorough search, though it can be set to True for faster results.\n",
    "    summary_ids = model.generate(inputs, max_length=2000, num_beams=30, early_stopping=False)\n",
    "\n",
    "    # Decode the generated IDs back to text, skipping special tokens like padding or EOS.\n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    # Return the generated summary\n",
    "    return summary\n",
    "\n",
    "# Prompt the user to enter text for summarization\n",
    "text = input('Enter the text: ')\n",
    "print()\n",
    "\n",
    "# Call the summarize_text function to generate a summary of the input text\n",
    "summary = summarize_text(text)\n",
    "\n",
    "# Print the generated summary\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agentic Tri model approach\n",
    "1. **Initial Summary Generation**: Use a pre-trained language model to generate an initial summary of the input document.\n",
    "2. **Instruction Generation**: Develop a smaller model or rule-based system to generate editing instructions based on user preferences.\n",
    "3. **Summary Refinement**: Apply the editing instructions to the initial summary to produce the final personalized summary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForSeq2SeqLM, pipeline\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "\n",
    "# Check for GPU availability\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "print(f\"Running on device: {device}\")\n",
    "\n",
    "# Load the Mistral-7B model for generation and editing.\n",
    "# (Replace \"mistral-7b\" with the actual Hugging Face model identifier if needed.)\n",
    "mistral_model_name = \"mistral-7b\"  \n",
    "mistral_tokenizer = AutoTokenizer.from_pretrained(mistral_model_name)\n",
    "mistral_model = AutoModelForCausalLM.from_pretrained(mistral_model_name, torch_dtype=torch.float16)\n",
    "mistral_model.to(\"cuda\" if device==0 else \"cpu\")\n",
    "\n",
    "# Create a text-generation pipeline using Mistral-7B for summarization tasks.\n",
    "summarizer = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=mistral_model,\n",
    "    tokenizer=mistral_tokenizer,\n",
    "    device=device,\n",
    "    max_new_tokens=150,  # adjust as needed\n",
    ")\n",
    "\n",
    "# Load the Flan-T5-large model for the instructor.\n",
    "instructor_model_name = \"google/flan-t5-large\"\n",
    "instructor = pipeline(\n",
    "    \"text2text-generation\",\n",
    "    model=instructor_model_name,\n",
    "    tokenizer=instructor_model_name,\n",
    "    device=device,\n",
    "    max_new_tokens=50,\n",
    ")\n",
    "\n",
    "def generate_initial_summary(document: str) -> str:\n",
    "    \"\"\"\n",
    "    Uses the Mistral-7B model to generate an initial summary for the given document.\n",
    "    \"\"\"\n",
    "    prompt = f\"Summarize the following document concisely:\\n\\n{document}\\n\\nSummary:\"\n",
    "    generated = summarizer(prompt, do_sample=True, temperature=0.7)[0][\"generated_text\"]\n",
    "    # Post-process to extract the summary portion\n",
    "    # (Here we assume the generated text starts with the prompt and then the summary)\n",
    "    summary = generated.split(\"Summary:\")[-1].strip()\n",
    "    return summary\n",
    "\n",
    "def generate_instructions(document: str, initial_summary: str) -> str:\n",
    "    \"\"\"\n",
    "    Uses the instructor (Flan-T5-large) to generate editing instructions\n",
    "    to improve the initial summary based on the document.\n",
    "    \"\"\"\n",
    "    prompt = (\n",
    "        f\"Given the document and its initial summary, generate concise editing instructions \"\n",
    "        f\"to improve factual consistency and coverage.\\n\\n\"\n",
    "        f\"Document:\\n{document}\\n\\n\"\n",
    "        f\"Initial Summary:\\n{initial_summary}\\n\\n\"\n",
    "        f\"Editing Instructions:\"\n",
    "    )\n",
    "    instructions = instructor(prompt)[0][\"generated_text\"].strip()\n",
    "    return instructions\n",
    "\n",
    "def edit_summary(document: str, initial_summary: str, instructions: str) -> str:\n",
    "    \"\"\"\n",
    "    Uses the Mistral-7B model to produce an edited summary based on the document,\n",
    "    the initial summary, and the editing instructions.\n",
    "    \"\"\"\n",
    "    prompt = (\n",
    "        f\"Edit the initial summary based on the following instructions to better match user preferences. \\n\\n\"\n",
    "        f\"Document:\\n{document}\\n\\n\"\n",
    "        f\"Initial Summary:\\n{initial_summary}\\n\\n\"\n",
    "        f\"Instructions:\\n{instructions}\\n\\n\"\n",
    "        f\"Edited Summary:\"\n",
    "    )\n",
    "    edited = summarizer(prompt, do_sample=True, temperature=0.7)[0][\"generated_text\"]\n",
    "    edited_summary = edited.split(\"Edited Summary:\")[-1].strip()\n",
    "    return edited_summary\n",
    "\n",
    "def compute_reward(generated_summary: str, human_summary: str, alpha: float = 0.5, beta: float = 0.5) -> float:\n",
    "    \"\"\"\n",
    "    Computes a reward score based on both ROUGE and BLEU metrics.\n",
    "    \n",
    "    - ROUGE: Computes the average of ROUGE-1 and ROUGE-L F1 scores.\n",
    "    - BLEU: Uses the sentence BLEU score.\n",
    "    \n",
    "    The final reward is a weighted sum of both metrics (default weights are 0.5 each).\n",
    "    \"\"\"\n",
    "    # Initialize ROUGE scorer\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
    "    rouge_scores = scorer.score(human_summary, generated_summary)\n",
    "    \n",
    "    # Average F1 score for ROUGE-1 and ROUGE-L\n",
    "    rouge_f1 = (rouge_scores['rouge1'].fmeasure + rouge_scores['rougeL'].fmeasure) / 2.0\n",
    "    \n",
    "    # Compute BLEU score (tokenized by simple whitespace split)\n",
    "    reference_tokens = human_summary.split()\n",
    "    candidate_tokens = generated_summary.split()\n",
    "    bleu_score = sentence_bleu([reference_tokens], candidate_tokens)\n",
    "    \n",
    "    # Combine both metrics into a reward (you can adjust alpha and beta weights as needed)\n",
    "    reward = alpha * rouge_f1 + beta * bleu_score\n",
    "    return reward\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document = (\n",
    "    \"NASA's Artemis I mission has successfully launched, marking the first step towards \"\n",
    "    \"returning humans to the Moon. The unmanned mission tested key systems and demonstrated \"\n",
    "    \"the capabilities of the new Space Launch System (SLS) rocket. NASA officials say the mission \"\n",
    "    \"will pave the way for future manned missions, including plans for a lunar orbiting platform \"\n",
    "    \"and eventual crewed landings on the Moon.\"\n",
    ")\n",
    "\n",
    "# Generate the initial summary using the generator (Mistral-7B)\n",
    "initial_summary = generate_initial_summary(document)\n",
    "print(\"Initial Summary:\")\n",
    "print(initial_summary)\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# Generate editing instructions using the instructor (Flan-T5-large)\n",
    "instructions = generate_instructions(document, initial_summary)\n",
    "print(\"Editing Instructions:\")\n",
    "print(instructions)\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# Generate the edited (refined) summary using the editor (Mistral-7B)\n",
    "edited_summary = edit_summary(document, initial_summary, instructions)\n",
    "print(\"Edited Summary:\")\n",
    "print(edited_summary)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another example run using the excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "excel_file = \"summaries_of_articles.xlsx\"\n",
    "df = pd.read_excel(excel_file)\n",
    "\n",
    "# For demonstration, we'll process the first few rows\n",
    "results = []\n",
    "for index, row in df.iterrows():\n",
    "    document = row[\"article_content\"]\n",
    "    human_summary = row[\"human_written_summary\"]\n",
    "    \n",
    "    # Generate initial summary using Mistral-7B\n",
    "    init_summary = generate_initial_summary(document)\n",
    "    \n",
    "    # Generate editing instructions using Flan-T5-large\n",
    "    instructions = generate_instructions(document, init_summary)\n",
    "    \n",
    "    # Generate the edited summary using Mistral-7B\n",
    "    edited_summary = edit_summary(document, init_summary, instructions)\n",
    "    \n",
    "    # Compute reward comparing the edited summary with the human-written summary\n",
    "    reward = compute_reward(edited_summary, human_summary)\n",
    "    \n",
    "    results.append({\n",
    "        \"document\": document,\n",
    "        \"human_summary\": human_summary,\n",
    "        \"initial_summary\": init_summary,\n",
    "        \"edited_summary\": edited_summary,\n",
    "        \"reward\": reward\n",
    "    })\n",
    "    \n",
    "    # For demonstration, process only a few rows; remove break to process all rows.\n",
    "    if index >= 2:\n",
    "        break\n",
    "\n",
    "# Convert results to a DataFrame and display\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)\n",
    "\n",
    "# Optionally, save the results to an Excel file for later inspection.\n",
    "results_df.to_excel(\"summaries_with_rewards.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deepseek code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "class OfflineMistralSummarizer:\n",
    "    def __init__(self, model_path=\"mistral-7b-instruct-v0.1.Q5_K_M.gguf\"):\n",
    "        # Initialize Mistral 7B GGUF model (download from Hugging Face)\n",
    "        self.llm = Llama(\n",
    "            model_path=model_path,\n",
    "            n_ctx=4096,  # Context window for long documents\n",
    "            n_gpu_layers=40,  # Offload to GPU if available\n",
    "            n_threads=8  # CPU threads\n",
    "        )\n",
    "        \n",
    "        # Initialize local instructor model (small Flan-T5 for comparison)\n",
    "        self.instructor_tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-base\")\n",
    "        self.instructor_model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-base\")\n",
    "\n",
    "    def generate_initial_summary(self, document):\n",
    "        \"\"\"Generator Agent: Mistral 7B produces initial summary\"\"\"\n",
    "        prompt = f\"\"\"<s>[INST] You are an AI summarizer. Create a concise summary of this document:\n",
    "        {document}\n",
    "        [/INST] Summary:\"\"\"\n",
    "        \n",
    "        response = self.llm(\n",
    "            prompt=prompt,\n",
    "            max_tokens=512,\n",
    "            temperature=0.7,\n",
    "            stop=[\"</s>\"]\n",
    "        )\n",
    "        return response['choices'][0]['text'].strip()\n",
    "\n",
    "    def generate_instructions(self, document, initial_summary, user_preferences):\n",
    "        \"\"\"Instructor Agent: Generate editing instructions using local model\"\"\"\n",
    "        input_text = f\"Document: {document}\\nSummary: {initial_summary}\\nPreferences: {user_preferences}\"\n",
    "        input_ids = self.instructor_tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
    "        \n",
    "        outputs = self.instructor_model.generate(input_ids)\n",
    "        return self.instructor_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    def edit_summary(self, document, initial_summary, instructions):\n",
    "        \"\"\"Editor Agent: Mistral 7B refines summary with instructions\"\"\"\n",
    "        prompt = f\"\"\"<s>[INST] Revise this summary based on the user's preferences:\n",
    "            Document: {document}\n",
    "            Original Summary: {initial_summary}\n",
    "            Editing Instructions: {instructions}\n",
    "            [/INST] Revised Summary:\"\"\"\n",
    "        \n",
    "        response = self.llm(\n",
    "            prompt=prompt,\n",
    "            max_tokens=1024,\n",
    "            temperature=0.5,\n",
    "            stop=[\"</s>\"]\n",
    "        )\n",
    "        return response['choices'][0]['text'].strip()\n",
    "\n",
    "    def calculate_reward(self, original_summary, edited_summary, user_preferences):\n",
    "        \"\"\"Simplified reward calculation (replace with your metrics)\"\"\"\n",
    "        # Overlap score\n",
    "        overlap = len(set(original_summary.split()) & set(edited_summary.split())) / len(set(original_summary.split()))\n",
    "        \n",
    "        # Preference score\n",
    "        pref_score = sum(1 for kw in user_preferences if kw.lower() in edited_summary.lower())\n",
    "        \n",
    "        return overlap + pref_score\n",
    "\n",
    "# Usage Example\n",
    "if __name__ == \"__main__\":\n",
    "    # First download the GGUF model from Hugging Face:\n",
    "    # wget https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF/resolve/main/mistral-7b-instruct-v0.1.Q5_K_M.gguf\n",
    "    \n",
    "    summarizer = OfflineMistralSummarizer(model_path=\"./mistral-7b-instruct-v0.1.Q5_K_M.gguf\")\n",
    "    \n",
    "    document = \"\"\"Machine learning (ML) is transforming industries through... [long text]...\"\"\"\n",
    "    user_prefs = [\"technical details\", \"model architectures\", \"performance metrics\"]\n",
    "    \n",
    "    # Generation pipeline\n",
    "    print(\"Generating initial summary...\")\n",
    "    initial = summarizer.generate_initial_summary(document)\n",
    "    print(\"Creating instructions...\")\n",
    "    instructions = summarizer.generate_instructions(document, initial, user_prefs)\n",
    "    print(\"Editing summary...\")\n",
    "    final = summarizer.edit_summary(document, initial, instructions)\n",
    "    \n",
    "    print(f\"\\nInitial Summary: {initial}\")\n",
    "    print(f\"\\nInstructions: {instructions}\")\n",
    "    print(f\"\\nFinal Summary: {final}\")\n",
    "    print(f\"\\nReward Score: {summarizer.calculate_reward(initial, final, user_prefs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chain of Thought approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load spaCy model for Named Entity Recognition (NER)\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Load a pre-trained summarization model\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "\n",
    "def extract_elements(text):\n",
    "    \"\"\"\n",
    "    Extract key elements from the text using Named Entity Recognition (NER).\n",
    "    \"\"\"\n",
    "    doc = nlp(text)\n",
    "    elements = {\n",
    "        \"entities\": [],\n",
    "        \"dates\": [],\n",
    "        \"events\": [],\n",
    "        \"locations\": []\n",
    "    }\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ in [\"PERSON\", \"ORG\"]:\n",
    "            elements[\"entities\"].append(ent.text)\n",
    "        elif ent.label_ == \"DATE\":\n",
    "            elements[\"dates\"].append(ent.text)\n",
    "        elif ent.label_ == \"GPE\":\n",
    "            elements[\"locations\"].append(ent.text)\n",
    "        # Events are not directly recognized by spaCy's NER,\n",
    "        # so we may need additional processing or a custom model.\n",
    "    return elements\n",
    "\n",
    "def generate_summary(text, elements):\n",
    "    \"\"\"\n",
    "    Generate a summary by incorporating extracted elements.\n",
    "    \"\"\"\n",
    "    # Initial summary generation\n",
    "    initial_summary = summarizer(text, max_length=150, min_length=40, do_sample=False)[0]['summary_text']\n",
    "    \n",
    "    # Incorporate extracted elements into the summary\n",
    "    # This is a simplified approach; more sophisticated methods can be applied.\n",
    "    summary = initial_summary\n",
    "    if elements[\"entities\"]:\n",
    "        summary += f\" Key entities involved: {', '.join(set(elements['entities']))}.\"\n",
    "    if elements[\"dates\"]:\n",
    "        summary += f\" Relevant dates: {', '.join(set(elements['dates']))}.\"\n",
    "    if elements[\"locations\"]:\n",
    "        summary += f\" Locations mentioned: {', '.join(set(elements['locations']))}.\"\n",
    "    # Events would be added here if extracted\n",
    "    \n",
    "    return summary\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    article = \"\"\"\n",
    "    The 69-year-old's Yamaha collided with a Nissan car between Handley's Corner and Barregarrow crossroads at about 17:00 BST on 4 June. Mr. Baker, who was from the island, was airlifted to Noble's Hospital, where he later died. The car driver, who police say was Northern Irish, was treated in hospital but has been discharged. Another motorcyclist who was injured after the crash has also been released from hospital.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Step 1: Extract elements\n",
    "    elements = extract_elements(article)\n",
    "    print(\"Extracted Elements:\", elements)\n",
    "    \n",
    "    # Step 2: Generate summary\n",
    "    summary = generate_summary(article, elements)\n",
    "    print(\"Generated Summary:\", summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine Tuning Tests (QLoRA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" ##specify the gpu you want to use\n",
    "\n",
    "import logging\n",
    "import torch\n",
    "from trl import SFTTrainer\n",
    "from datasets import Dataset, DatasetDict, load_metric, load_dataset\n",
    "from peft import LoraConfig, AutoPeftModelForCausalLM, prepare_model_for_kbit_training, get_peft_model, PeftModel\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments, Trainer, AutoConfig\n",
    "\n",
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "        Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params, all_param = 0, 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    logger.info(f\"Trainable params: {trainable_params} || All params: {all_param} || Trainable (%): {100 * trainable_params / all_param}\")\n",
    "\n",
    "def load_model(model_path):\n",
    "    model_name = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "\n",
    "    if not model_path:\n",
    "        model_path = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "    \n",
    "    compute_dtype = getattr(torch, \"float16\")\n",
    "    bnb_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_use_double_quant=False, bnb_4bit_quant_type=\"nf4\",\n",
    "                                    bnb_4bit_compute_dtype=compute_dtype)\n",
    "\n",
    "    # Check GPU compatibility with bfloat16\n",
    "    if compute_dtype == torch.float16:\n",
    "        major, _ = torch.cuda.get_device_capability()\n",
    "        if major >= 8:\n",
    "            logger.info(\"=\" * 80)\n",
    "            logger.info(\"GPU supports bfloat16: accelerate training with bf16=True\")\n",
    "            logger.info(\"=\" * 80)    \n",
    "\n",
    "    \n",
    "    # load model and tokenizer\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_path, quantization_config=bnb_config, device_map=\"auto\",\n",
    "                                                 trust_remote_code=True, attn_implementation=\"flash_attention_2\")\n",
    "    model.config.use_cache = False\n",
    "    model.config.pretraining_tp = 1\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, model_max_length=4096, add_eos_token=True)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    # Fix weird overflow issue with fp16 training\n",
    "    tokenizer.padding_side = \"right\" \n",
    "\n",
    "    # lora config file\n",
    "    config = LoraConfig(\n",
    "      r=32, # Sets the rank of the LoRA adaptation, which controls the size of low-rank updates.\n",
    "      lora_alpha=16, # Scaling factor for the LoRA weights.\n",
    "      bias=\"none\", \n",
    "      lora_dropout=0.1, \n",
    "      target_modules=[ # Specifies the layers to which LoRA will be applied (e.g., projection layers in the attention and feedforward network).\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\", \"lm_head\"\n",
    "      ],\n",
    "      task_type=\"CAUSAL_LM\"\n",
    "    )\n",
    "\n",
    "    model = get_peft_model(model, config)\n",
    "    print_trainable_parameters(model)\n",
    "\n",
    "    # Apply the accelerator.\n",
    "    # model = accelerator.prepare_model(model)\n",
    "\n",
    "    model.gradient_checkpointing_enable()\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "    # Load LoRA configuration\n",
    "    peft_config = LoraConfig(r=8, lora_alpha=16, lora_dropout=0.1,  bias=\"none\", task_type=\"CAUSAL_LM\")\n",
    "    \n",
    "    return model, tokenizer, peft_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing QLoRA for fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "def train_args(output_dir, dataset, model_path=None):\n",
    "    \n",
    "    logger.info(f\"Loading model ....\")\n",
    "    model, tokenizer, peft_config = load_model(model_path)\n",
    "    \n",
    "    training_arguments = TrainingArguments(\n",
    "    output_dir=output_dir, \n",
    "    num_train_epochs=5, \n",
    "    per_device_train_batch_size=1, # batch size for training is 1 per GPU\n",
    "    gradient_accumulation_steps=2, # gradients will be accumulated over 2 steps\n",
    "    gradient_checkpointing = True, \n",
    "    save_steps=1000, # save model every {save_steps} steps\n",
    "    logging_steps=200, \n",
    "    max_steps=-1, # -1 indicates \"no step limit\"\n",
    "    optim=\"paged_adamw_32bit\", # Uses the 32-bit Paged AdamW optimizer, an optimized version of the AdamW optimizer, suited for training in lower precision\n",
    "    learning_rate=2e-4, \n",
    "    weight_decay=0.001, \n",
    "    fp16=False, bf16=True, # Training is done in bfloat16 precision (bf16=True), which is more stable than FP16 in terms of numerical accuracy but still provides the speed and memory efficiency advantages. \n",
    "    max_grad_norm=0.3, # limit grad's norm to 0.3 to prevent exploding gradient\n",
    "    warmup_ratio=0.03, #  Specifies a warmup period where the learning rate gradually increases during the first {warmup_ratio}% of training\n",
    "    group_by_length=True, \n",
    "    lr_scheduler_type=\"cosine\" # Uses a cosine learning rate scheduler, which reduces the learning rate following a cosine curve after the warmup phase.\n",
    "    )\n",
    "      \n",
    "    # Pack multiple short examples in the same input sequence to increase efficiency\n",
    "    packing = False\n",
    "    \n",
    "    # Set supervised fine-tuning parameters\n",
    "    trainer = SFTTrainer(\n",
    "    model=model, \n",
    "    train_dataset=dataset, \n",
    "    peft_config=peft_config, \n",
    "    dataset_text_field=\"text\", # Specifies that the text data is contained in a field named \"text\" in the dataset.\n",
    "    max_seq_length=4096, # Sets the maximum sequence length to 4096 tokens, which is the upper limit for Mistral models.\n",
    "    tokenizer=tokenizer, \n",
    "    args=training_arguments, \n",
    "    packing=False # Disables packing, meaning that sequences will not be combined during training.\n",
    "    )\n",
    "\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call functions and train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"./models/\"\n",
    "dataset = get_dataset('training_data.csv')\n",
    "model_path = None\n",
    "\n",
    "trainer = train_args(output_dir, dataset, model_path)\n",
    "\n",
    "# Train model\n",
    "trainer.train()\n",
    "\n",
    "# Save trained model\n",
    "output_dir = os.path.join(output_dir, \"final_ckpt\")\n",
    "trainer.model.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the fine tuned mistral model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(ckpt_path, model_name):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "    tokenizer.pad_token = tokenizer.unk_token\n",
    "    tokenizer.padding_side = \"right\"  \n",
    "    \n",
    "    streamer = TextIteratorStreamer(tokenizer, skip_prompt=True, timeout=20.0, skip_special_tokens=True)\n",
    "    model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "      ckpt_path, \n",
    "      low_cpu_mem_usage=True, \n",
    "      torch_dtype=torch.bfloat16,\n",
    "      load_in_4bit=True\n",
    "    )\n",
    "\n",
    "    logger.info(f\"Model loaded successfully\")\n",
    "    return model, tokenizer, streamer\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "ckpt_path = './nlp_exp/mistral_ckpts/checkpoint-20000'\n",
    "\n",
    "model, tokenizer, streamer = load_model(ckpt_path, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference(prompts, model, tokenizer):\n",
    "    model_inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True, max_length=16384).to(\"cuda\")\n",
    "    print(f\"Number of tokens in the input string: {model_inputs['input_ids'].shape}\")\n",
    "    \n",
    "    with torch.inference_mode():\n",
    "        outputs = model.generate(\n",
    "            **model_inputs, # The tokenized input prompts are passed to the model for generation.\n",
    "            max_new_tokens=1250, #  The model will generate up to 1250 new tokens. This sets the upper limit for the length of the generated text.\n",
    "            repetition_penalty=1.2, # This parameter penalizes repeated tokens during generation, encouraging the model to avoid repetitive text.\n",
    "            pad_token_id=tokenizer.pad_token_id \n",
    "        )\n",
    "    \n",
    "    final_text = tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0][len(prompts):]\n",
    "    return final_text.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing out Reinforcement Learning with Human Feedback (RLHF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0286e80eae91429f82cdcf8afa260255",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATASET_FOLDER = f\"C:\\Users\\ianwa\\Downloads\\archive\\cnn_dailymail\"\n",
    "SAMPLE_5_ARTICLES_FILE = r\"D:\\PersonalProjs\\newsTrainer\\summariser\\sample_5_articles.xlsx\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "# Load data\n",
    "df = pd.read_excel(SAMPLE_5_ARTICLES_FILE)\n",
    "articles = df[\"article_content\"].tolist()\n",
    "human_summaries = df[\"human_written_summary\"].tolist()\n",
    "machine_summaries = df[\"machine_written_summary\"].tolist()\n",
    "\n",
    "# Create preference dataset\n",
    "data = {\n",
    "    \"article\": articles,\n",
    "    \"chosen\": human_summaries,\n",
    "    \"rejected\": machine_summaries\n",
    "}\n",
    "dataset = Dataset.from_dict(data).train_test_split(test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['article', 'chosen', 'rejected'],\n",
       "        num_rows: 5\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['article', 'chosen', 'rejected'],\n",
       "        num_rows: 1\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from trl import DPOConfig, DPOTrainer\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "import torch\n",
    "\n",
    "training_args = DPOConfig(\n",
    "    output_dir=\"models/dpo\", \n",
    "    logging_steps=25,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    save_strategy=\"epoch\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    eval_steps=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ianwa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\accelerate\\utils\\modeling.py:1390: UserWarning: Current model requires 8192 bytes of buffer for offloaded layers, which seems does not fit any GPU's remaining memory. If you are experiencing a OOM later, please consider using offload_buffers=True.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cfa5f8927b142e6bf1b24d37eb8ddbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the disk and cpu.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, TrainingArguments\n",
    "from trl import RewardTrainer\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments, Trainer, AutoConfig\n",
    "import torch\n",
    "\n",
    "# Load model\n",
    "model_id = \"mistralai/Mistral-7B-v0.1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "compute_dtype = getattr(torch, \"float16\")\n",
    "bnb_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\",\n",
    "                                bnb_4bit_compute_dtype=compute_dtype)\n",
    "# Prepare model for reward training\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    num_labels=1,\n",
    "    # quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "282f1a3495fe423eb527808c42b2edd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting prompt in train dataset:   0%|          | 0/5 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47e50b2350b742588ad0ecdddd80b4b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying chat template to train dataset:   0%|          | 0/5 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4d9ba8770194899a938c4e7b613e600",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/5 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed9da047687c4fc497809e090464211b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting prompt in eval dataset:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c7248354b614e7faac77dcef6e6966a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying chat template to eval dataset:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a496310ff1bc49e7aa016b758153f740",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing eval dataset:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You shouldn't move a model that is dispatched using accelerate hooks.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "You can't move a model that has some modules offloaded to cpu or disk.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[43mDPOTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprocessing_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtest\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\utils\\deprecation.py:165\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action\u001b[38;5;241m.\u001b[39mNOTIFY, Action\u001b[38;5;241m.\u001b[39mNOTIFY_ALWAYS):\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[0;32m    163\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m--> 165\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\trl\\trainer\\dpo_trainer.py:470\u001b[0m, in \u001b[0;36mDPOTrainer.__init__\u001b[1;34m(self, model, ref_model, args, data_collator, train_dataset, eval_dataset, processing_class, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics, peft_config)\u001b[0m\n\u001b[0;32m    467\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    468\u001b[0m         eval_dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_dataset(eval_dataset, processing_class, args, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meval\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 470\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    471\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    472\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    473\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_collator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_collator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    474\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    475\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    476\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprocessing_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprocessing_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    477\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    478\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompute_metrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    479\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    480\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    481\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreprocess_logits_for_metrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreprocess_logits_for_metrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    482\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    484\u001b[0m \u001b[38;5;66;03m# Gradient accumulation requires scaled loss. Normally, loss scaling in the parent class depends on whether the\u001b[39;00m\n\u001b[0;32m    485\u001b[0m \u001b[38;5;66;03m# model accepts loss-related kwargs. Since we compute our own loss, this check is irrelevant. We set\u001b[39;00m\n\u001b[0;32m    486\u001b[0m \u001b[38;5;66;03m# self.model_accepts_loss_kwargs to False to enable scaling.\u001b[39;00m\n\u001b[0;32m    487\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_accepts_loss_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\utils\\deprecation.py:165\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action\u001b[38;5;241m.\u001b[39mNOTIFY, Action\u001b[38;5;241m.\u001b[39mNOTIFY_ALWAYS):\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[0;32m    163\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m--> 165\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\trainer.py:574\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[1;34m(self, model, args, data_collator, train_dataset, eval_dataset, processing_class, model_init, compute_loss_func, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics)\u001b[0m\n\u001b[0;32m    569\u001b[0m \u001b[38;5;66;03m# Bnb Quantized models doesn't support `.to` operation.\u001b[39;00m\n\u001b[0;32m    570\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    571\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mplace_model_on_device\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquantization_method\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m QuantizationMethod\u001b[38;5;241m.\u001b[39mBITS_AND_BYTES\n\u001b[0;32m    573\u001b[0m ):\n\u001b[1;32m--> 574\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_move_model_to_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    576\u001b[0m \u001b[38;5;66;03m# Force n_gpu to 1 to avoid DataParallel as MP will manage the GPUs\u001b[39;00m\n\u001b[0;32m    577\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_model_parallel:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\trainer.py:846\u001b[0m, in \u001b[0;36mTrainer._move_model_to_device\u001b[1;34m(self, model, device)\u001b[0m\n\u001b[0;32m    845\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_move_model_to_device\u001b[39m(\u001b[38;5;28mself\u001b[39m, model, device):\n\u001b[1;32m--> 846\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    847\u001b[0m     \u001b[38;5;66;03m# Moving a model to an XLA device disconnects the tied weights, so we have to retie them.\u001b[39;00m\n\u001b[0;32m    848\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mparallel_mode \u001b[38;5;241m==\u001b[39m ParallelMode\u001b[38;5;241m.\u001b[39mTPU \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtie_weights\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\accelerate\\big_modeling.py:456\u001b[0m, in \u001b[0;36mdispatch_model.<locals>.add_warning.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    454\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m param \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mparameters():\n\u001b[0;32m    455\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m param\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeta\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 456\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt move a model that has some modules offloaded to cpu or disk.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    457\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: You can't move a model that has some modules offloaded to cpu or disk."
     ]
    }
   ],
   "source": [
    "trainer = DPOTrainer(\n",
    "    model=model, \n",
    "    args=training_args, \n",
    "    processing_class=tokenizer, \n",
    "    train_dataset=dataset['train'],\n",
    "    eval_dataset=dataset['test'],\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ec64db2921446d1b393996fa3ae6e5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ianwa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:653: UserWarning: Not enough free disk space to download the file. The expected file size is: 9942.98 MB. The target location C:\\Users\\ianwa\\.cache\\huggingface\\hub\\models--mistralai--Mistral-7B-v0.1\\blobs only has 5515.60 MB free disk space.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac4543628ed64a869387db9f90fd9625",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   6%|5         | 587M/9.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 28] No space left on device",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m bnb_config \u001b[38;5;241m=\u001b[39m BitsAndBytesConfig(load_in_4bit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, bnb_4bit_use_double_quant\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, bnb_4bit_quant_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnf4\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     13\u001b[0m                                 bnb_4bit_compute_dtype\u001b[38;5;241m=\u001b[39mcompute_dtype)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Prepare model for reward training\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m reward_model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_labels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# quantization_config=bnb_config,\u001b[39;49;00m\n\u001b[0;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m     20\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Tokenization function\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtokenize_function\u001b[39m(examples):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:564\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    562\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m    563\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[1;32m--> 564\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    565\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    566\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    567\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    569\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    570\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\modeling_utils.py:3974\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   3971\u001b[0m \u001b[38;5;66;03m# We'll need to download and cache each checkpoint shard if the checkpoint is sharded.\u001b[39;00m\n\u001b[0;32m   3972\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_sharded:\n\u001b[0;32m   3973\u001b[0m     \u001b[38;5;66;03m# resolved_archive_file becomes a list of files that point to the different checkpoint shards in this case.\u001b[39;00m\n\u001b[1;32m-> 3974\u001b[0m     resolved_archive_file, sharded_metadata \u001b[38;5;241m=\u001b[39m \u001b[43mget_checkpoint_shard_files\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3975\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3976\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresolved_archive_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3977\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3978\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3979\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3980\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3981\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3982\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3983\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3984\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3985\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3986\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3987\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3989\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   3990\u001b[0m     is_safetensors_available()\n\u001b[0;32m   3991\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resolved_archive_file, \u001b[38;5;28mstr\u001b[39m)\n\u001b[0;32m   3992\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m resolved_archive_file\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.safetensors\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   3993\u001b[0m ):\n\u001b[0;32m   3994\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m safe_open(resolved_archive_file, framework\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\utils\\hub.py:1098\u001b[0m, in \u001b[0;36mget_checkpoint_shard_files\u001b[1;34m(pretrained_model_name_or_path, index_filename, cache_dir, force_download, proxies, resume_download, local_files_only, token, user_agent, revision, subfolder, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[0;32m   1095\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m shard_filename \u001b[38;5;129;01min\u001b[39;00m tqdm(shard_filenames, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownloading shards\u001b[39m\u001b[38;5;124m\"\u001b[39m, disable\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m show_progress_bar):\n\u001b[0;32m   1096\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1097\u001b[0m         \u001b[38;5;66;03m# Load from URL\u001b[39;00m\n\u001b[1;32m-> 1098\u001b[0m         cached_filename \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1099\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1100\u001b[0m \u001b[43m            \u001b[49m\u001b[43mshard_filename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1101\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1102\u001b[0m \u001b[43m            \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1103\u001b[0m \u001b[43m            \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1104\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1105\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1106\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1107\u001b[0m \u001b[43m            \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1108\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1109\u001b[0m \u001b[43m            \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1110\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_commit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1111\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1112\u001b[0m     \u001b[38;5;66;03m# We have already dealt with RepositoryNotFoundError and RevisionNotFoundError when getting the index, so\u001b[39;00m\n\u001b[0;32m   1113\u001b[0m     \u001b[38;5;66;03m# we don't have to catch them here.\u001b[39;00m\n\u001b[0;32m   1114\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\utils\\hub.py:403\u001b[0m, in \u001b[0;36mcached_file\u001b[1;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[0;32m    400\u001b[0m user_agent \u001b[38;5;241m=\u001b[39m http_user_agent(user_agent)\n\u001b[0;32m    401\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    402\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[1;32m--> 403\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    404\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    405\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    406\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    407\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    409\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    411\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    412\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    414\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    415\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    416\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    417\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    418\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m _get_cache_file_to_return(path_or_repo_id, full_filename, cache_dir, revision)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[0;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:862\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[1;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[0;32m    842\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _hf_hub_download_to_local_dir(\n\u001b[0;32m    843\u001b[0m         \u001b[38;5;66;03m# Destination\u001b[39;00m\n\u001b[0;32m    844\u001b[0m         local_dir\u001b[38;5;241m=\u001b[39mlocal_dir,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    859\u001b[0m         local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[0;32m    860\u001b[0m     )\n\u001b[0;32m    861\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 862\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_hf_hub_download_to_cache_dir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    863\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Destination\u001b[39;49;00m\n\u001b[0;32m    864\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    865\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# File info\u001b[39;49;00m\n\u001b[0;32m    866\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    867\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    868\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    869\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    870\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# HTTP info\u001b[39;49;00m\n\u001b[0;32m    871\u001b[0m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    872\u001b[0m \u001b[43m        \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    873\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Additional options\u001b[39;49;00m\n\u001b[0;32m    877\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:1011\u001b[0m, in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[1;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[0m\n\u001b[0;32m   1009\u001b[0m Path(lock_path)\u001b[38;5;241m.\u001b[39mparent\u001b[38;5;241m.\u001b[39mmkdir(parents\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   1010\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m WeakFileLock(lock_path):\n\u001b[1;32m-> 1011\u001b[0m     \u001b[43m_download_to_tmp_and_move\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1012\u001b[0m \u001b[43m        \u001b[49m\u001b[43mincomplete_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblob_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.incomplete\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1013\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdestination_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblob_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1014\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl_to_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl_to_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1015\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1016\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1017\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexpected_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1018\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1019\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1020\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1021\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(pointer_path):\n\u001b[0;32m   1022\u001b[0m         _create_symlink(blob_path, pointer_path, new_blob\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:1545\u001b[0m, in \u001b[0;36m_download_to_tmp_and_move\u001b[1;34m(incomplete_path, destination_path, url_to_download, proxies, headers, expected_size, filename, force_download)\u001b[0m\n\u001b[0;32m   1542\u001b[0m         _check_disk_space(expected_size, incomplete_path\u001b[38;5;241m.\u001b[39mparent)\n\u001b[0;32m   1543\u001b[0m         _check_disk_space(expected_size, destination_path\u001b[38;5;241m.\u001b[39mparent)\n\u001b[1;32m-> 1545\u001b[0m     \u001b[43mhttp_get\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1546\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl_to_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1547\u001b[0m \u001b[43m        \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1548\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1549\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1550\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1551\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexpected_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1552\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1554\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownload complete. Moving file to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdestination_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1555\u001b[0m _chmod_and_move(incomplete_path, destination_path)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:457\u001b[0m, in \u001b[0;36mhttp_get\u001b[1;34m(url, temp_file, proxies, resume_size, headers, expected_size, displayed_filename, _nb_retries, _tqdm_bar)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunk:  \u001b[38;5;66;03m# filter out keep-alive new chunks\u001b[39;00m\n\u001b[0;32m    456\u001b[0m     progress\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mlen\u001b[39m(chunk))\n\u001b[1;32m--> 457\u001b[0m     \u001b[43mtemp_file\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    458\u001b[0m     new_resume_size \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(chunk)\n\u001b[0;32m    459\u001b[0m     \u001b[38;5;66;03m# Some data has been downloaded from the server so we reset the number of retries.\u001b[39;00m\n",
      "\u001b[1;31mOSError\u001b[0m: [Errno 28] No space left on device"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, TrainingArguments\n",
    "from trl import RewardTrainer\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments, Trainer, AutoConfig\n",
    "\n",
    "\n",
    "# Load model\n",
    "model_id = \"mistralai/Mistral-7B-v0.1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "compute_dtype = getattr(torch, \"float16\")\n",
    "bnb_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_use_double_quant=False, bnb_4bit_quant_type=\"nf4\",\n",
    "                                bnb_4bit_compute_dtype=compute_dtype)\n",
    "# Prepare model for reward training\n",
    "reward_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    num_labels=1,\n",
    "    # quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Tokenization function\n",
    "def tokenize_function(examples):\n",
    "    tokenized_articles = tokenizer(examples[\"article\"], padding=\"max_length\", truncation=True, max_length=512)\n",
    "    \n",
    "    tokenized_chosen = tokenizer(\n",
    "        examples[\"chosen\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=256,\n",
    "    )\n",
    "    \n",
    "    tokenized_rejected = tokenizer(\n",
    "        examples[\"rejected\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=256,\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        \"input_ids_article\": tokenized_articles[\"input_ids\"],\n",
    "        \"attention_mask_article\": tokenized_articles[\"attention_mask\"],\n",
    "        \"input_ids_chosen\": tokenized_chosen[\"input_ids\"],\n",
    "        \"attention_mask_chosen\": tokenized_chosen[\"attention_mask\"],\n",
    "        \"input_ids_rejected\": tokenized_rejected[\"input_ids\"],\n",
    "        \"attention_mask_rejected\": tokenized_rejected[\"attention_mask\"],\n",
    "    }\n",
    "\n",
    "# Prepare dataset\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Training setup\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"reward_model\",\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=1e-5,\n",
    "    num_train_epochs=3,\n",
    ")\n",
    "\n",
    "trainer = RewardTrainer(\n",
    "    model=reward_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import PPOTrainer, PPOConfig, AutoModelForCausalLMWithValueHead\n",
    "from peft import LoraConfig\n",
    "\n",
    "# Load base model with PEFT/LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "ppo_model = AutoModelForCausalLMWithValueHead.from_pretrained(\n",
    "    model_id,\n",
    "    load_in_4bit=True,\n",
    "    device_map=\"auto\",\n",
    "    peft_config=lora_config\n",
    ")\n",
    "\n",
    "ppo_config = PPOConfig(\n",
    "    batch_size=1,\n",
    "    learning_rate=1e-5,\n",
    "    mini_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    ")\n",
    "\n",
    "ppo_trainer = PPOTrainer(\n",
    "    model=ppo_model,\n",
    "    config=ppo_config,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# Training loop\n",
    "for article, human_summary in zip(articles, human_summaries):\n",
    "    # Generate summary\n",
    "    inputs = tokenizer(article, return_tensors=\"pt\").to(\"cuda\")\n",
    "    generation_config = {\n",
    "        \"max_length\": 256,\n",
    "        \"do_sample\": True,\n",
    "        \"temperature\": 0.7,\n",
    "        \"top_p\": 0.9\n",
    "    }\n",
    "    \n",
    "    summary_tokens = ppo_model.generate(**inputs, **generation_config)\n",
    "    generated_summary = tokenizer.decode(summary_tokens[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Get reward\n",
    "    reward_inputs = tokenizer(\n",
    "        article,\n",
    "        generated_summary,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True\n",
    "    ).to(\"cuda\")\n",
    "    \n",
    "    reward = reward_model(**reward_inputs).logits[0].detach()\n",
    "    \n",
    "    # PPO update\n",
    "    ppo_trainer.step([summary_tokens], [reward])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ianwa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments\n",
    "from trl import PPOTrainer, AutoModelForCausalLMWithValueHead\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from rouge_score import rouge_scorer\n",
    "from tqdm import tqdm\n",
    "\n",
    "class SummaryDataset:\n",
    "    def __init__(self, excel_path):\n",
    "        # Load Excel file\n",
    "        df = pd.read_excel(excel_path)\n",
    "        \n",
    "        # Extract relevant columns\n",
    "        self.articles = df['article_content'].tolist()\n",
    "        self.human_summaries = df['human_written_summary'].tolist()\n",
    "        self.machine_summaries = df['machine_written_summary'].tolist()\n",
    "        self.titles = df['article_title'].tolist()\n",
    "        self.dates = df['date'].tolist()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.articles)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'article': self.articles[idx],\n",
    "            'human_summary': self.human_summaries[idx],\n",
    "            'machine_summary': self.machine_summaries[idx],\n",
    "            'title': self.titles[idx],\n",
    "            'date': self.dates[idx]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize dataset\n",
    "dataset = SummaryDataset(SAMPLE_5_ARTICLES_FILE)\n",
    "\n",
    "# 2. Initialize Models ---------------------------------------------------------\n",
    "model_name = \"mistralai/Mistral-7B-v0.1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForSequenceClassification\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "class RewardModel(nn.Module):\n",
    "    \"\"\"Reward model that learns to predict the quality of summaries based on human feedback.\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name=\"distilroberta-base\"): # distilled ver of Robustly Optimised BERT Approach\n",
    "        super(RewardModel, self).__init__()\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=1)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        return outputs.logits\n",
    "    \n",
    "    def tokenize(self, articles, summaries):\n",
    "        inputs = [f\"Article: {article}\\nSummary: {summary}\" for article, summary in zip(articles, summaries)]\n",
    "        return self.tokenizer(inputs, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "    \n",
    "    def get_reward(self, articles, summaries):\n",
    "        inputs = self.tokenize(articles, summaries)\n",
    "        with torch.no_grad():\n",
    "            rewards = self.forward(inputs[\"input_ids\"], inputs[\"attention_mask\"])\n",
    "        return rewards.squeeze(-1)\n",
    "\n",
    "def prepare_reward_data(articles, human_summaries, model_summaries):\n",
    "    \"\"\"Prepare paired data for training the reward model.\"\"\"\n",
    "    paired_data = {\n",
    "        \"articles\": [],\n",
    "        \"chosen_summaries\": [],\n",
    "        \"rejected_summaries\": [],\n",
    "    }\n",
    "    \n",
    "    for article, human_summary, model_summary in zip(articles, human_summaries, model_summaries):\n",
    "        paired_data[\"articles\"].append(article)\n",
    "        paired_data[\"chosen_summaries\"].append(human_summary)\n",
    "        paired_data[\"rejected_summaries\"].append(model_summary)\n",
    "    \n",
    "    return paired_data\n",
    "\n",
    "def train_reward_model(reward_model, train_data, val_data, epochs=3, batch_size=8, lr=2e-5):\n",
    "    \"\"\"Train the reward model on human preferences.\"\"\"\n",
    "    train_dataset = Dataset.from_dict(train_data)\n",
    "    val_dataset = Dataset.from_dict(val_data)\n",
    "    \n",
    "    def collate_fn(batch):\n",
    "        articles = [item[\"articles\"] for item in batch]\n",
    "        chosen = [item[\"chosen_summaries\"] for item in batch]\n",
    "        rejected = [item[\"rejected_summaries\"] for item in batch]\n",
    "        \n",
    "        chosen_inputs = reward_model.tokenize(articles, chosen)\n",
    "        rejected_inputs = reward_model.tokenize(articles, rejected)\n",
    "        \n",
    "        return {\n",
    "            \"chosen_input_ids\": chosen_inputs[\"input_ids\"],\n",
    "            \"chosen_attention_mask\": chosen_inputs[\"attention_mask\"],\n",
    "            \"rejected_input_ids\": rejected_inputs[\"input_ids\"],\n",
    "            \"rejected_attention_mask\": rejected_inputs[\"attention_mask\"],\n",
    "        }\n",
    "    \n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=batch_size, collate_fn=collate_fn)\n",
    "    \n",
    "    optimizer = optim.AdamW(reward_model.parameters(), lr=lr)\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    reward_model.to(device)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        reward_model.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        for batch in tqdm(train_dataloader, desc=f\"Epoch {epoch+1}\"):\n",
    "            chosen_rewards = reward_model(\n",
    "                batch[\"chosen_input_ids\"].to(device),\n",
    "                batch[\"chosen_attention_mask\"].to(device)\n",
    "            )\n",
    "            rejected_rewards = reward_model(\n",
    "                batch[\"rejected_input_ids\"].to(device),\n",
    "                batch[\"rejected_attention_mask\"].to(device)\n",
    "            )\n",
    "            \n",
    "            # Bradley-Terry preference loss: log(σ(chosen_reward - rejected_reward))\n",
    "            loss = -torch.log(torch.sigmoid(chosen_rewards - rejected_rewards)).mean()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        # Validation\n",
    "        reward_model.eval()\n",
    "        val_loss = 0\n",
    "        correct = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_dataloader:\n",
    "                chosen_rewards = reward_model(\n",
    "                    batch[\"chosen_input_ids\"].to(device),\n",
    "                    batch[\"chosen_attention_mask\"].to(device)\n",
    "                )\n",
    "                rejected_rewards = reward_model(\n",
    "                    batch[\"rejected_input_ids\"].to(device),\n",
    "                    batch[\"rejected_attention_mask\"].to(device)\n",
    "                )\n",
    "                \n",
    "                loss = -torch.log(torch.sigmoid(chosen_rewards - rejected_rewards)).mean()\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                # Count correct predictions (chosen > rejected)\n",
    "                correct += (chosen_rewards > rejected_rewards).sum().item()\n",
    "        \n",
    "        val_accuracy = correct / len(val_dataset)\n",
    "        print(f\"Epoch {epoch+1}: Train Loss: {total_loss / len(train_dataloader):.4f}, \"\n",
    "              f\"Val Loss: {val_loss / len(val_dataloader):.4f}, Val Accuracy: {val_accuracy:.4f}\")\n",
    "    \n",
    "    return reward_model\n",
    "\n",
    "class PPOSummarizer:\n",
    "    \"\"\"PPO-based summarization model that learns from the reward model.\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name=\"mistralai/Mistral-7B-Instruct-v0.1\", reward_model=None):\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        self.reward_model = reward_model\n",
    "        \n",
    "        # Clone model for reference (to calculate KL divergence)\n",
    "        self.ref_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "        \n",
    "        # Freeze reference model\n",
    "        for param in self.ref_model.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    def generate_summaries(self, articles, max_length=150):\n",
    "        \"\"\"Generate summaries using the current policy.\"\"\"\n",
    "        summaries = []\n",
    "        \n",
    "        for article in articles:\n",
    "            prompt = f\"Summarize the following article:\\n\\n{article}\\n\\nSummary:\"\n",
    "            inputs = self.tokenizer(prompt, return_tensors=\"pt\", padding=True)\n",
    "            \n",
    "            output_ids = self.model.generate(\n",
    "                inputs[\"input_ids\"],\n",
    "                max_length=inputs[\"input_ids\"].shape[1] + max_length,\n",
    "                attention_mask=inputs[\"attention_mask\"],\n",
    "                do_sample=True,\n",
    "                top_p=0.9,\n",
    "                temperature=0.7\n",
    "            )\n",
    "            \n",
    "            summary = self.tokenizer.decode(output_ids[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True)\n",
    "            summaries.append(summary)\n",
    "            \n",
    "        return summaries\n",
    "    \n",
    "    def compute_kl_divergence(self, states, actions, action_masks):\n",
    "        \"\"\"Compute KL divergence between current policy and reference policy.\"\"\"\n",
    "        with torch.no_grad():\n",
    "            ref_logits = self.ref_model(input_ids=states, attention_mask=action_masks).logits\n",
    "        \n",
    "        logits = self.model(input_ids=states, attention_mask=action_masks).logits\n",
    "        \n",
    "        # Get logits for actions\n",
    "        action_indices = torch.arange(actions.shape[0]).unsqueeze(1)\n",
    "        action_logprobs = torch.log_softmax(logits, dim=-1).gather(-1, actions.unsqueeze(-1)).squeeze(-1)\n",
    "        ref_action_logprobs = torch.log_softmax(ref_logits, dim=-1).gather(-1, actions.unsqueeze(-1)).squeeze(-1)\n",
    "        \n",
    "        # Calculate KL\n",
    "        kl = (action_logprobs - ref_action_logprobs) * action_masks\n",
    "        kl = kl.sum(dim=1) / action_masks.sum(dim=1)\n",
    "        \n",
    "        return kl\n",
    "    \n",
    "    def ppo_train(self, articles, batch_size=4, epochs=1, ppo_epochs=4, \n",
    "                 clip_param=0.2, value_coef=0.5, entropy_coef=0.01, max_grad_norm=0.5,\n",
    "                 lr=1e-5, kl_coef=0.1):\n",
    "        \"\"\"Train the summarizer using PPO algorithm.\"\"\"\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model.to(device)\n",
    "        self.ref_model.to(device)\n",
    "        self.reward_model.to(device)\n",
    "        \n",
    "        optimizer = optim.AdamW(self.model.parameters(), lr=lr)\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Sample trajectories using current policy\n",
    "            all_rewards = []\n",
    "            all_advantages = []\n",
    "            all_logprobs = []\n",
    "            all_values = []\n",
    "            all_states = []\n",
    "            all_actions = []\n",
    "            all_action_masks = []\n",
    "            \n",
    "            for i in range(0, len(articles), batch_size):\n",
    "                batch_articles = articles[i:i+batch_size]\n",
    "                \n",
    "                # Generate summaries\n",
    "                with torch.no_grad():\n",
    "                    summaries = self.generate_summaries(batch_articles)\n",
    "                \n",
    "                # Compute rewards\n",
    "                with torch.no_grad():\n",
    "                    rewards = self.reward_model.get_reward(batch_articles, summaries)\n",
    "                \n",
    "                # Tokenize for processing\n",
    "                prompts = [f\"Summarize the following article:\\n\\n{article}\\n\\nSummary:\" for article in batch_articles]\n",
    "                prompt_encodings = self.tokenizer(prompts, return_tensors=\"pt\", padding=True).to(device)\n",
    "                \n",
    "                summary_encodings = self.tokenizer(summaries, return_tensors=\"pt\", padding=True).to(device)\n",
    "                actions = summary_encodings[\"input_ids\"]\n",
    "                action_masks = summary_encodings[\"attention_mask\"]\n",
    "                \n",
    "                # Compute logprobs and values\n",
    "                with torch.no_grad():\n",
    "                    outputs = self.model(input_ids=prompt_encodings[\"input_ids\"], \n",
    "                                         attention_mask=prompt_encodings[\"attention_mask\"])\n",
    "                    logprobs = torch.log_softmax(outputs.logits, dim=-1)\n",
    "                    values = torch.zeros_like(rewards)  # Simplified, would need a value head\n",
    "                \n",
    "                # Store trajectory data\n",
    "                all_rewards.extend(rewards.tolist())\n",
    "                all_advantages.extend(rewards.tolist())  # Simplified, would normally compute proper advantages\n",
    "                all_logprobs.append(logprobs)\n",
    "                all_values.append(values)\n",
    "                all_states.append(prompt_encodings[\"input_ids\"])\n",
    "                all_actions.append(actions)\n",
    "                all_action_masks.append(action_masks)\n",
    "            \n",
    "            # PPO update\n",
    "            for _ in range(ppo_epochs):\n",
    "                for i in range(len(all_rewards)):\n",
    "                    states = all_states[i]\n",
    "                    actions = all_actions[i]\n",
    "                    action_masks = all_action_masks[i]\n",
    "                    old_logprobs = all_logprobs[i]\n",
    "                    advantages = torch.tensor(all_advantages[i], device=device)\n",
    "                    \n",
    "                    # Get current logprobs\n",
    "                    outputs = self.model(input_ids=states, attention_mask=action_masks)\n",
    "                    logprobs = torch.log_softmax(outputs.logits, dim=-1)\n",
    "                    \n",
    "                    # Compute ratio and clipped loss\n",
    "                    ratio = torch.exp(logprobs - old_logprobs)\n",
    "                    clipped_ratio = torch.clamp(ratio, 1 - clip_param, 1 + clip_param)\n",
    "                    \n",
    "                    # Compute KL divergence for regularization\n",
    "                    kl_div = self.compute_kl_divergence(states, actions, action_masks)\n",
    "                    \n",
    "                    # Calculate losses\n",
    "                    policy_loss = -torch.min(ratio * advantages, clipped_ratio * advantages).mean()\n",
    "                    value_loss = 0  # Simplified, would need proper value head\n",
    "                    entropy_loss = -entropy_coef * torch.mean(torch.exp(logprobs) * logprobs)\n",
    "                    kl_loss = kl_coef * kl_div.mean()\n",
    "                    \n",
    "                    # Total loss\n",
    "                    loss = policy_loss + value_coef * value_loss - entropy_loss + kl_loss\n",
    "                    \n",
    "                    # Update\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_grad_norm)\n",
    "                    optimizer.step()\n",
    "            \n",
    "            print(f\"Epoch {epoch+1} completed. Average reward: {np.mean(all_rewards):.4f}\")\n",
    "\n",
    "def enhance_with_rlhf(articles, human_summaries, extract_elements_fn, generate_summary_fn):\n",
    "    \"\"\"Main function to enhance summary generation with RLHF.\"\"\"\n",
    "    \n",
    "    # Step 1: Generate model summaries using your existing functions\n",
    "    model_summaries = []\n",
    "    for article in articles:\n",
    "        elements = extract_elements_fn(article)\n",
    "        summary = generate_summary_fn(elements)\n",
    "        model_summaries.append(summary)\n",
    "    \n",
    "    # Step 2: Split data for reward model training\n",
    "    train_indices, val_indices = train_test_split(range(len(articles)), test_size=0.2)\n",
    "    \n",
    "    train_articles = [articles[i] for i in train_indices]\n",
    "    train_human_summaries = [human_summaries[i] for i in train_indices]\n",
    "    train_model_summaries = [model_summaries[i] for i in train_indices]\n",
    "    \n",
    "    val_articles = [articles[i] for i in val_indices]\n",
    "    val_human_summaries = [human_summaries[i] for i in val_indices]\n",
    "    val_model_summaries = [model_summaries[i] for i in val_indices]\n",
    "    \n",
    "    # Step 3: Prepare preference data\n",
    "    train_data = prepare_reward_data(train_articles, train_human_summaries, train_model_summaries)\n",
    "    val_data = prepare_reward_data(val_articles, val_human_summaries, val_model_summaries)\n",
    "    \n",
    "    # Step 4: Train the reward model\n",
    "    print(\"Training reward model...\")\n",
    "    reward_model = RewardModel()\n",
    "    trained_reward_model = train_reward_model(reward_model, train_data, val_data)\n",
    "    \n",
    "    # Step 5: Train the summarizer with PPO\n",
    "    print(\"Training summarizer with PPO...\")\n",
    "    ppo_summarizer = PPOSummarizer(reward_model=trained_reward_model)\n",
    "    ppo_summarizer.ppo_train(articles)\n",
    "    \n",
    "    # Step 6: Create an enhanced generate_summary function that uses the RL-trained model\n",
    "    def enhanced_generate_summary(elements, article=None):\n",
    "        if article is None:\n",
    "            # Fallback to original function if no article provided\n",
    "            return generate_summary_fn(elements)\n",
    "        \n",
    "        # Use the RL-trained model to generate a summary\n",
    "        prompt = f\"Summarize the following article considering these key elements: {elements}\\n\\n{article}\\n\\nSummary:\"\n",
    "        inputs = ppo_summarizer.tokenizer(prompt, return_tensors=\"pt\")\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output_ids = ppo_summarizer.model.generate(\n",
    "                inputs[\"input_ids\"],\n",
    "                max_length=inputs[\"input_ids\"].shape[1] + 150,\n",
    "                do_sample=True,\n",
    "                top_p=0.9,\n",
    "                temperature=0.7\n",
    "            )\n",
    "        \n",
    "        summary = ppo_summarizer.tokenizer.decode(output_ids[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True)\n",
    "        return summary\n",
    "    \n",
    "    return enhanced_generate_summary, trained_reward_model, ppo_summarizer\n",
    "\n",
    "# Example usage\n",
    "\"\"\"\n",
    "# You would use it like this with your existing functions\n",
    "enhanced_generate_summary, reward_model, ppo_summarizer = enhance_with_rlhf(\n",
    "    articles, \n",
    "    human_summaries, \n",
    "    extract_elements,  # Your existing function\n",
    "    generate_summary   # Your existing function\n",
    ")\n",
    "\n",
    "# Now you can use the enhanced version\n",
    "article = \"...\"\n",
    "elements = extract_elements(article)\n",
    "improved_summary = enhanced_generate_summary(elements, article)\n",
    "\"\"\"\n",
    "\n",
    "# For evaluation purposes\n",
    "def evaluate_summaries(articles, gold_summaries, generated_summaries, reward_model=None):\n",
    "    \"\"\"Evaluate summaries using ROUGE metrics and optional reward model.\"\"\"\n",
    "    from rouge import Rouge\n",
    "    \n",
    "    rouge = Rouge()\n",
    "    scores = rouge.get_scores(generated_summaries, gold_summaries, avg=True)\n",
    "    \n",
    "    reward_scores = None\n",
    "    if reward_model:\n",
    "        with torch.no_grad():\n",
    "            reward_scores = reward_model.get_reward(articles, generated_summaries).mean().item()\n",
    "    \n",
    "    results = {\n",
    "        \"ROUGE-1\": scores[\"rouge-1\"][\"f\"],\n",
    "        \"ROUGE-2\": scores[\"rouge-2\"][\"f\"],\n",
    "        \"ROUGE-L\": scores[\"rouge-l\"][\"f\"],\n",
    "    }\n",
    "    \n",
    "    if reward_scores:\n",
    "        results[\"Reward Score\"] = reward_scores\n",
    "    \n",
    "    return results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
